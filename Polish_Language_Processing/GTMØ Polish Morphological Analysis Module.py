#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GTMÃ˜ Polish NLP Implementation - NAPRAWIONA WERSJA
==================================================
Kompletna implementacja GTMÃ˜ dla jÄ™zyka polskiego z naprawionÄ… instalacjÄ…
optimized for Google Colab environment.

Author: GTMÃ˜ Research Team
Version: 2.1 - FIXED Installation Issues
Date: 2024
"""

import subprocess
import sys
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import json
import logging
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# NAPRAWIONA INSTALACJA ZALEÅ»NOÅšCI
# ============================================================================

def install_dependencies():
    """Naprawiona instalacja z lepszÄ… obsÅ‚ugÄ… bÅ‚Ä™dÃ³w."""
    print("ðŸ”§ Konfiguracja Å›rodowiska Google Colab...")

    # Lista podstawowych pakietÃ³w
    basic_packages = [
        'plotly',
        'kaleido',  # For plotly export
        'pandas',
        'numpy'
    ]

    # Zainstaluj podstawowe pakiety
    for package in basic_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"âœ… {package} juÅ¼ zainstalowany")
        except ImportError:
            print(f"ðŸ“¦ InstalujÄ™ {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])
            print(f"âœ… {package} zainstalowany")

    # SprÃ³buj zainstalowaÄ‡ Morfeusz2 z kilkoma metodami
    morfeusz_installed = install_morfeusz2()

    # Zainstaluj spaCy i model polski
    spacy_installed = install_spacy_polish()

    # Zainstaluj Stanza (opcjonalnie)
    stanza_installed = install_stanza_polish()

    print("\nðŸ“Š Podsumowanie instalacji:")
    print(f"   ðŸ“¦ Podstawowe pakiety: âœ…")
    print(f"   ðŸ”¤ Morfeusz2: {'âœ…' if morfeusz_installed else 'âŒ (bÄ™dzie uÅ¼ywany fallback)'}")
    print(f"   ðŸ§  spaCy: {'âœ…' if spacy_installed else 'âŒ (bÄ™dzie uÅ¼ywany fallback)'}")
    print(f"   ðŸ“š Stanza: {'âœ…' if stanza_installed else 'âŒ (bÄ™dzie uÅ¼ywany fallback)'}")

    return {
        'morfeusz': morfeusz_installed,
        'spacy': spacy_installed,
        'stanza': stanza_installed
    }

def install_morfeusz2():
    """Naprawiona instalacja Morfeusz2 z wieloma metodami."""
    print("ðŸ”¤ PrÃ³bujÄ™ zainstalowaÄ‡ Morfeusz2...")

    # Metoda 1: PrÃ³ba standardowej instalacji
    try:
        import morfeusz2
        print("âœ… Morfeusz2 juÅ¼ dostÄ™pny")
        return True
    except ImportError:
        pass

    # Metoda 2: pip install morfeusz2
    try:
        print("   ðŸ“¦ Metoda 1: pip install morfeusz2...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "morfeusz2"])
        import morfeusz2
        print("âœ… Morfeusz2 zainstalowany przez pip")
        return True
    except:
        print("   âŒ Metoda 1 nieudana")

    # Metoda 3: BezpoÅ›redni wheel
    wheel_urls = [
        "https://github.com/morfeusz-project/morfeusz/releases/download/2.1.9/morfeusz2-2.1.9-py3-none-any.whl",
        "https://github.com/morfeusz-project/morfeusz/releases/download/2.0.9/morfeusz2-2.0.9-py3-none-any.whl"
    ]

    for url in wheel_urls:
        try:
            print(f"   ðŸ“¦ Metoda 2: PrÃ³bujÄ™ {url}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", url])
            import morfeusz2
            print("âœ… Morfeusz2 zainstalowany z wheel")
            return True
        except Exception as e:
            print(f"   âŒ BÅ‚Ä…d: {str(e)[:100]}")
            continue

    # Metoda 4: Instalacja zaleÅ¼noÅ›ci systemowych
    try:
        print("   ðŸ“¦ Metoda 3: InstalujÄ™ zaleÅ¼noÅ›ci systemowe...")
        subprocess.check_call(["apt-get", "update", "-qq"])
        subprocess.check_call(["apt-get", "install", "-y", "-qq", "python3-dev", "build-essential"])
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "morfeusz2"])
        import morfeusz2
        print("âœ… Morfeusz2 zainstalowany z zaleÅ¼noÅ›ciami")
        return True
    except:
        print("   âŒ Metoda 3 nieudana")

    print("âš ï¸ Morfeusz2 niedostÄ™pny - system bÄ™dzie uÅ¼ywaÅ‚ fallback")
    return False

def install_spacy_polish():
    """Instalacja spaCy z polskim modelem."""
    print("ðŸ§  InstalujÄ™ spaCy...")

    try:
        import spacy
        # SprawdÅº czy model polski jest dostÄ™pny
        try:
            nlp = spacy.load("pl_core_news_sm")
            print("âœ… spaCy z modelem polskim juÅ¼ dostÄ™pny")
            return True
        except:
            pass
    except ImportError:
        # Zainstaluj spaCy
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "spacy"])
            import spacy
        except:
            print("âŒ Nie moÅ¼na zainstalowaÄ‡ spaCy")
            return False

    # Zainstaluj model polski
    models_to_try = ["pl_core_news_sm", "pl_core_news_md", "pl_core_news_lg"]

    for model in models_to_try:
        try:
            print(f"   ðŸ“š Pobieram model {model}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model, "--quiet"])

            # Test czy model dziaÅ‚a
            import spacy
            nlp = spacy.load(model)
            print(f"âœ… spaCy model {model} zainstalowany")
            return True
        except Exception as e:
            print(f"   âŒ Model {model} nieudany: {str(e)[:50]}")
            continue

    print("âš ï¸ spaCy zainstalowany ale bez polskiego modelu")
    return False

def install_stanza_polish():
    """Instalacja Stanza (opcjonalna)."""
    print("ðŸ“š InstalujÄ™ Stanza...")

    try:
        # Zainstaluj Stanza
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "stanza"])

        # SprawdÅº czy dziaÅ‚a
        import stanza
        print("âœ… Stanza zainstalowana (model zostanie pobrany przy pierwszym uÅ¼yciu)")
        return True
    except Exception as e:
        print(f"âŒ Stanza installation failed: {str(e)[:50]}")
        return False

# Uruchom instalacjÄ™ na poczÄ…tku komÃ³rki
print("=" * 60)
print("GTMÃ˜ POLISH NLP - NAPRAWIONA INSTALACJA")
print("=" * 60)

# Store installation status globally for access in other parts of the script
global installation_status
installation_status = install_dependencies()


# ============================================================================
# IMPORTY I KONFIGURACJA
# ============================================================================

import numpy as np
import pandas as pd

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("âš ï¸ Plotly niedostÄ™pny - wizualizacje wyÅ‚Ä…czone")

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# PODSTAWOWE KLASY GTMÃ˜
# ============================================================================

PHI = (1 + np.sqrt(5)) / 2  # Golden ratio
SQRT_2_INV = 1 / np.sqrt(2)  # Quantum amplitude
COGNITIVE_CENTER = np.array([0.5, 0.5, 0.5])  # Neutral knowledge state

# ============================================================================
# GTMÃ˜ WSPÃ“ÅRZÄ˜DNE DLA CECH MORFOLOGICZNYCH
# ============================================================================

# Aspekt czasownika (determination, stability, entropy)
ASPECT_COORDS = {
    'perf': (0.8, 0.7, 0.2),    # Dokonany = okreÅ›lony, zakoÅ„czony
    'imperf': (0.5, 0.5, 0.5)   # Niedokonany = trwaÅ‚y, niezakoÅ„czony
}

# Czas czasownika
TENSE_COORDS = {
    'past': (0.7, 0.8, 0.2),    # PrzeszÅ‚y = stabilny, okreÅ›lony
    'pres': (0.6, 0.5, 0.4),    # TeraÅºniejszy = mniej stabilny
    'fut': (0.4, 0.3, 0.6)      # PrzyszÅ‚y = niepewny, wysoka entropia
}

# Liczba gramatyczna
NUMBER_COORDS = {
    'sg': (0.7, 0.7, 0.3),      # Pojedyncza = bardziej okreÅ›lona
    'pl': (0.5, 0.5, 0.5)       # Mnoga = bardziej ogÃ³lna
}

# Rodzaj gramatyczny
GENDER_COORDS = {
    'm1': (0.8, 0.7, 0.3),      # MÄ™skoosobowy = okreÅ›lony
    'm2': (0.7, 0.6, 0.4),      # MÄ™ski Å¼ywotny
    'm3': (0.6, 0.6, 0.4),      # MÄ™ski nieÅ¼ywotny
    'f': (0.7, 0.7, 0.3),       # Å»eÅ„ski = okreÅ›lony
    'n': (0.6, 0.5, 0.5)        # Nijaki = mniej okreÅ›lony
}

# StopieÅ„ przymiotnika
DEGREE_COORDS = {
    'pos': (0.6, 0.7, 0.3),     # RÃ³wny = normalny stopieÅ„
    'com': (0.5, 0.5, 0.5),     # WyÅ¼szy = porÃ³wnawczy
    'sup': (0.8, 0.6, 0.4)      # NajwyÅ¼szy = najbardziej okreÅ›lony
}

@dataclass
class GTMOCoordinates:
    """WspÃ³Å‚rzÄ™dne GTMÃ˜ w przestrzeni fazowej 3D."""
    determination: float = 0.5
    stability: float = 0.5
    entropy: float = 0.5

    def to_array(self) -> np.ndarray:
        return np.array([self.determination, self.stability, self.entropy])

    def distance_to(self, other: 'GTMOCoordinates') -> float:
        return np.linalg.norm(self.to_array() - other.to_array())

@dataclass
class Configuration:
    """Kompletna konfiguracja w Przestrzeni Konfiguracyjnej GTMÃ˜."""
    position: np.ndarray = field(default_factory=lambda: np.zeros(3))
    time: float = 0.0
    orientation: np.ndarray = field(default_factory=lambda: np.array([0., 0., 0.]))
    scale: float = 1.0

    def to_dict(self) -> Dict:
        return {
            'position': self.position.tolist(),
            'time': self.time,
            'orientation': self.orientation.tolist(),
            'scale': self.scale
        }

class PolishCase(Enum):
    """Polskie przypadki gramatyczne z wspÃ³Å‚rzÄ™dnymi GTMÃ˜."""
    NOMINATIVE = ("nom", "mianownik", 0.901, 0.799, 0.151)
    GENITIVE = ("gen", "dopeÅ‚niacz", 0.701, 0.751, 0.301)
    DATIVE = ("dat", "celownik", 0.651, 0.549, 0.451)
    ACCUSATIVE = ("acc", "biernik", 0.851, 0.801, 0.201)
    INSTRUMENTAL = ("inst", "narzÄ™dnik", 0.499, 0.301, 0.899)
    LOCATIVE = ("loc", "miejscownik", 0.501, 0.499, 0.801)
    VOCATIVE = ("voc", "woÅ‚acz", 0.149, 0.151, 0.851)

    def __init__(self, tag: str, polish_name: str,
                 determination: float, stability: float, entropy: float):
        self.tag = tag
        self.polish_name = polish_name
        self.coords = GTMOCoordinates(determination, stability, entropy)

class PolishPOS(Enum):
    """Polish parts of speech with GTMÃ˜ coordinates."""
    SUBST = ("subst", "rzeczownik", 0.8, 0.9, 0.2)
    ADJ = ("adj", "przymiotnik", 0.6, 0.5, 0.4)
    ADV = ("adv", "przysÅ‚Ã³wek", 0.5, 0.6, 0.4)
    VERB = ("verb", "czasownik", 0.7, 0.4, 0.5)
    NUM = ("num", "liczebnik", 0.9, 0.8, 0.1)
    PRON = ("pron", "zaimek", 0.8, 0.6, 0.3)
    PREP = ("prep", "przyimek", 0.6, 0.8, 0.3)
    CONJ = ("conj", "spÃ³jnik", 0.5, 0.7, 0.4)
    PART = ("part", "partykuÅ‚a", 0.3, 0.2, 0.8)
    INTERP = ("interp", "interpunkcja", 0.9, 0.9, 0.1)

    def __init__(self, tag: str, polish_name: str,
                 determination: float, stability: float, entropy: float):
        self.tag = tag
        self.polish_name = polish_name
        self.coords = GTMOCoordinates(determination, stability, entropy)

class KnowledgeAttractor(Enum):
    """Atraktory wiedzy GTMÃ˜ do klasyfikacji."""
    PARTICLE = ("Î¨á´·", "Knowledge Particle", 0.85, 0.85, 0.15)
    SHADOW = ("Î¨Ê°", "Knowledge Shadow", 0.15, 0.15, 0.85)
    EMERGENT = ("Î¨á´º", "Emergent", 0.5, 0.3, 0.9)
    SINGULARITY = ("Ã˜", "Singularity", 1.0, 1.0, 0.0)
    FLUX = ("Î¨~", "Flux", 0.5, 0.5, 0.8)
    VOID = ("Î¨â—Š", "Void", 0.0, 0.0, 0.5)

    def __init__(self, symbol: str, display_name: str,
                 determination: float, stability: float, entropy: float):
        self.symbol = symbol
        self.display_name = display_name
        self.coords = GTMOCoordinates(determination, stability, entropy)

# ============================================================================
# SÅOWNIK NIEREGULARNYCH FORM
# ============================================================================

class IrregularFormsDict:
    """SÅ‚ownik nieregularnych form czasownikÃ³w polskich."""

    def __init__(self):
        """Inicjalizuj sÅ‚ownik z pliku JSON."""
        self.irregular_verbs = {}
        self.form_to_lemma = {}  # Mapowanie forma â†’ lemma
        self._load_irregular_verbs()

    def _load_irregular_verbs(self):
        """Wczytaj nieregularne czasowniki z pliku JSON."""
        import json
        import os

        # ÅšcieÅ¼ka do pliku JSON
        data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
        json_path = os.path.join(data_dir, 'polish_irregular_verbs.json')

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                self.irregular_verbs = json.load(f)

            # Zbuduj mapowanie forma â†’ lemma
            for lemma, data in self.irregular_verbs.items():
                for tense, forms in data.get('forms', {}).items():
                    for person, form in forms.items():
                        # ObsÅ‚uga form zÅ‚oÅ¼onych (np. "bÄ™dÄ™ miaÅ‚")
                        main_form = form.split()[0] if ' ' in form else form
                        self.form_to_lemma[main_form.lower()] = lemma

            logger.info(f"âœ… Wczytano {len(self.irregular_verbs)} nieregularnych czasownikÃ³w")

        except FileNotFoundError:
            logger.warning(f"âš ï¸ Brak pliku {json_path} - sÅ‚ownik nieregularny niedostÄ™pny")
        except Exception as e:
            logger.warning(f"âš ï¸ BÅ‚Ä…d wczytywania sÅ‚ownika: {e}")

    def lookup(self, word: str) -> Optional[Dict[str, Any]]:
        """
        SprawdÅº czy sÅ‚owo jest nieregularnÄ… formÄ… czasownika.

        Returns:
            Dict z lemma, aspect, gtmo_coords lub None
        """
        word_lower = word.lower()

        if word_lower in self.form_to_lemma:
            lemma = self.form_to_lemma[word_lower]
            verb_data = self.irregular_verbs.get(lemma, {})

            return {
                'lemma': lemma,
                'aspect': verb_data.get('aspect', 'imperf'),
                'gtmo_coords': verb_data.get('gtmo_coords', {}),
                'is_irregular': True,
                'type': 'irregular_verb'
            }

        return None

    def get_all_forms(self, lemma: str) -> Optional[Dict]:
        """Pobierz wszystkie formy dla danego lematu."""
        return self.irregular_verbs.get(lemma)

# ============================================================================
# ANALIZATOR DERYWACYJNY (Prefiksy/Sufiksy)
# ============================================================================

class DerivationalAnalyzer:
    """Analizator derywacyjny dla jÄ™zyka polskiego (sÅ‚owotwÃ³rstwo)."""

    def __init__(self):
        """Inicjalizuj analizator z danymi o afiksach."""
        self.prefixes = {}
        self.suffixes = {}
        self._load_affixes()

    def _load_affixes(self):
        """Wczytaj prefiksy i sufiksy z pliku JSON."""
        import json
        import os

        data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
        json_path = os.path.join(data_dir, 'polish_derivational_affixes.json')

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.prefixes = data.get('prefixes', {})
                self.suffixes = data.get('suffixes', {})

            logger.info(f"âœ… Wczytano {len(self.prefixes)} prefiksÃ³w i {len(self.suffixes)} sufiksÃ³w")

        except FileNotFoundError:
            logger.warning(f"âš ï¸ Brak pliku {json_path} - analiza derywacyjna niedostÄ™pna")
        except Exception as e:
            logger.warning(f"âš ï¸ BÅ‚Ä…d wczytywania afiksÃ³w: {e}")

    def extract_prefix(self, word: str) -> Optional[Tuple[str, str, Dict]]:
        """
        WyciÄ…gnij prefiks ze sÅ‚owa.

        Returns:
            (prefix, stem, prefix_data) lub None
        """
        word_lower = word.lower()

        # Sortuj prefiksy od najdÅ‚uÅ¼szych do najkrÃ³tszych
        sorted_prefixes = sorted(self.prefixes.keys(), key=len, reverse=True)

        for prefix in sorted_prefixes:
            if word_lower.startswith(prefix) and len(word_lower) > len(prefix) + 2:
                stem = word_lower[len(prefix):]
                prefix_data = self.prefixes[prefix]
                return (prefix, stem, prefix_data)

        return None

    def extract_suffix(self, word: str) -> Optional[Tuple[str, str, Dict]]:
        """
        WyciÄ…gnij sufiks ze sÅ‚owa.

        Returns:
            (suffix, stem, suffix_data) lub None
        """
        word_lower = word.lower()

        # Sortuj sufiksy od najdÅ‚uÅ¼szych do najkrÃ³tszych
        sorted_suffixes = sorted(self.suffixes.keys(), key=len, reverse=True)

        for suffix in sorted_suffixes:
            if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:
                stem = word_lower[:-len(suffix)]
                suffix_data = self.suffixes[suffix]
                return (suffix, stem, suffix_data)

        return None

    def analyze_word(self, word: str) -> Dict[str, Any]:
        """
        PeÅ‚na analiza derywacyjna sÅ‚owa.

        Returns:
            Dict z informacjami o prefiksie, sufiksie i wpÅ‚ywie na GTMÃ˜
        """
        result = {
            'word': word,
            'has_prefix': False,
            'has_suffix': False,
            'prefix': None,
            'suffix': None,
            'stem': word,
            'gtmo_modifications': {
                'determination': 0.0,
                'stability': 0.0,
                'entropy': 0.0
            }
        }

        # Analiza prefiksu
        prefix_info = self.extract_prefix(word)
        if prefix_info:
            prefix, stem, prefix_data = prefix_info
            result['has_prefix'] = True
            result['prefix'] = {
                'text': prefix,
                'type': prefix_data.get('type'),
                'examples': prefix_data.get('examples', [])
            }
            result['stem'] = stem

            # Dodaj wpÅ‚yw na GTMÃ˜
            gtmo_effect = prefix_data.get('gtmo_effect', {})
            for coord, value in gtmo_effect.items():
                result['gtmo_modifications'][coord] += value

        # Analiza sufiksu
        suffix_info = self.extract_suffix(word)
        if suffix_info:
            suffix, stem, suffix_data = suffix_info
            result['has_suffix'] = True
            result['suffix'] = {
                'text': suffix,
                'derives': suffix_data.get('derives'),
                'from': suffix_data.get('from'),
                'examples': suffix_data.get('examples', [])
            }

            # JeÅ›li nie byÅ‚o prefiksu, ustaw stem z sufiksu
            if not result['has_prefix']:
                result['stem'] = stem

            # Dodaj wpÅ‚yw na GTMÃ˜
            gtmo_effect = suffix_data.get('gtmo_effect', {})
            for coord, value in gtmo_effect.items():
                result['gtmo_modifications'][coord] += value

        return result

# ============================================================================
# ANALIZATOR KOLOKACJI
# ============================================================================

class CollocationAnalyzer:
    """Analizator kolokacji i idiomÃ³w w jÄ™zyku polskim."""

    def __init__(self):
        """Inicjalizuj analizator z danymi o kolokacjach."""
        self.collocations = {}
        self.all_patterns = []  # Lista wszystkich wzorcÃ³w (dla szybkiego przeszukiwania)
        self._load_collocations()

    def _load_collocations(self):
        """Wczytaj kolokacje z pliku JSON."""
        import json
        import os

        data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
        json_path = os.path.join(data_dir, 'polish_collocations.json')

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.collocations = data

            # Zbuduj pÅ‚askÄ… listÄ™ wszystkich wzorcÃ³w
            for category, patterns in data.items():
                for pattern, info in patterns.items():
                    self.all_patterns.append({
                        'pattern': pattern,
                        'category': category,
                        'info': info
                    })

            logger.info(f"âœ… Wczytano {len(self.all_patterns)} kolokacji")

        except FileNotFoundError:
            logger.warning(f"âš ï¸ Brak pliku {json_path} - analiza kolokacji niedostÄ™pna")
        except Exception as e:
            logger.warning(f"âš ï¸ BÅ‚Ä…d wczytywania kolokacji: {e}")

    def find_collocations(self, text: str) -> List[Dict[str, Any]]:
        """
        ZnajdÅº kolokacje w tekÅ›cie.

        Returns:
            Lista znalezionych kolokacji z informacjami
        """
        text_lower = text.lower()
        found_collocations = []

        for pattern_info in self.all_patterns:
            pattern = pattern_info['pattern']

            if pattern in text_lower:
                found_collocations.append({
                    'pattern': pattern,
                    'category': pattern_info['category'],
                    'type': pattern_info['info'].get('type'),
                    'frequency': pattern_info['info'].get('frequency'),
                    'gtmo_coords': pattern_info['info'].get('gtmo_coords', {}),
                    'meaning': pattern_info['info'].get('meaning')  # Dla idiomÃ³w
                })

        return found_collocations

    def calculate_collocation_effect(self, collocations: List[Dict]) -> Dict[str, float]:
        """
        Oblicz zagregowany wpÅ‚yw kolokacji na wspÃ³Å‚rzÄ™dne GTMÃ˜.

        Returns:
            Dict z modyfikacjami wspÃ³Å‚rzÄ™dnych
        """
        if not collocations:
            return {'determination': 0.0, 'stability': 0.0, 'entropy': 0.0}

        # UÅ›rednij wpÅ‚yw wszystkich znalezionych kolokacji
        total_det = sum(c['gtmo_coords'].get('determination', 0.5) for c in collocations)
        total_stab = sum(c['gtmo_coords'].get('stability', 0.5) for c in collocations)
        total_ent = sum(c['gtmo_coords'].get('entropy', 0.5) for c in collocations)

        count = len(collocations)

        return {
            'determination': total_det / count,
            'stability': total_stab / count,
            'entropy': total_ent / count
        }

# ============================================================================
# SEMANTIC EMBEDDING ANALYZER (Framework)
# ============================================================================

class SemanticEmbeddingAnalyzer:
    """
    Framework do analizy semantycznej z embeddings.

    Obecnie uÅ¼ywa sÅ‚ownika podobieÅ„stw, ale gotowy do rozszerzenia o:
    - word2vec
    - fastText
    - BERT/transformers
    """

    def __init__(self, use_pretrained=False):
        """
        Inicjalizuj analizator.

        Args:
            use_pretrained: Czy uÅ¼ywaÄ‡ pretrenowanych modeli (word2vec/fasttext)
        """
        self.use_pretrained = use_pretrained
        self.model = None
        self.semantic_clusters = {}
        self.word_similarities = {}
        self._load_semantic_data()

        if use_pretrained:
            self._try_load_pretrained()

    def _load_semantic_data(self):
        """Wczytaj podstawowe dane semantyczne ze sÅ‚ownika."""
        import json
        import os

        data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
        json_path = os.path.join(data_dir, 'polish_semantic_similarities.json')

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.semantic_clusters = data.get('semantic_clusters', {})
                self.word_similarities = data.get('word_similarities', {})

            logger.info(f"âœ… Wczytano {len(self.semantic_clusters)} klastrÃ³w semantycznych")

        except FileNotFoundError:
            logger.warning(f"âš ï¸ Brak pliku {json_path} - analiza semantyczna niedostÄ™pna")
        except Exception as e:
            logger.warning(f"âš ï¸ BÅ‚Ä…d wczytywania danych semantycznych: {e}")

    def _try_load_pretrained(self):
        """SprÃ³buj zaÅ‚adowaÄ‡ pretrenowany model (word2vec/fasttext)."""
        try:
            # PrÃ³ba zaÅ‚adowania gensim
            import gensim
            logger.info("âš ï¸ Gensim dostÄ™pny - moÅ¼na zaÅ‚adowaÄ‡ pretrenowane modele")
            logger.info("   Dodaj model word2vec dla polskiego do data/word2vec_polish.bin")
            # self.model = gensim.models.KeyedVectors.load_word2vec_format(...)
        except ImportError:
            logger.info("â„¹ï¸ Gensim niedostÄ™pny - uÅ¼ywam sÅ‚ownika podobieÅ„stw")

    def find_semantic_cluster(self, word: str) -> Optional[Dict]:
        """
        ZnajdÅº klaster semantyczny dla sÅ‚owa.

        Returns:
            Dict z informacjami o klastrze i wspÃ³Å‚rzÄ™dnych GTMÃ˜
        """
        word_lower = word.lower()

        for cluster_name, cluster_data in self.semantic_clusters.items():
            if word_lower in cluster_data.get('words', []):
                return {
                    'cluster': cluster_name,
                    'gtmo_coords': cluster_data.get('gtmo_coords', {}),
                    'words': cluster_data.get('words', [])
                }

        return None

    def get_similar_words(self, word: str, top_n=5) -> List[str]:
        """
        Pobierz podobne sÅ‚owa.

        Returns:
            Lista podobnych sÅ‚Ã³w
        """
        word_lower = word.lower()

        # JeÅ›li dostÄ™pny pretrenowany model
        if self.model is not None:
            try:
                similar = self.model.most_similar(word_lower, topn=top_n)
                return [w[0] for w in similar]
            except:
                pass

        # Fallback: uÅ¼yj sÅ‚ownika
        return self.word_similarities.get(word_lower, [])[:top_n]

    def calculate_semantic_context(self, words: List[str]) -> Dict[str, float]:
        """
        Oblicz kontekst semantyczny na podstawie listy sÅ‚Ã³w.

        Returns:
            Dict z zagregowanymi wspÃ³Å‚rzÄ™dnymi GTMÃ˜
        """
        cluster_coords = []

        for word in words:
            cluster = self.find_semantic_cluster(word)
            if cluster:
                cluster_coords.append(cluster['gtmo_coords'])

        if not cluster_coords:
            return {'determination': 0.5, 'stability': 0.5, 'entropy': 0.5}

        # UÅ›rednij wspÃ³Å‚rzÄ™dne
        avg_det = sum(c.get('determination', 0.5) for c in cluster_coords) / len(cluster_coords)
        avg_stab = sum(c.get('stability', 0.5) for c in cluster_coords) / len(cluster_coords)
        avg_ent = sum(c.get('entropy', 0.5) for c in cluster_coords) / len(cluster_coords)

        return {
            'determination': avg_det,
            'stability': avg_stab,
            'entropy': avg_ent
        }

# ============================================================================
# ANALIZATOR MORFOLOGICZNY Z FALLBACK
# ============================================================================

class MorfeuszAnalyzer:
    """Analizator morfologiczny Morfeusz2 z fallback - TYLKO MORFOLOGIA."""

    def __init__(self):
        # Check global installation status
        if installation_status.get('morfeusz'):
            try:
                import morfeusz2
                self.morfeusz = morfeusz2.Morfeusz()
                self.available = True
                logger.info("âœ… Morfeusz2 zainicjalizowany")
            except Exception as e:
                self.morfeusz = None
                self.available = False
                logger.warning(f"âŒ Morfeusz2 bÅ‚Ä…d: {e}")
        else:
            self.morfeusz = None
            self.available = False
            logger.info("â„¹ï¸ Morfeusz2 niedostÄ™pny - uÅ¼ywam fallback")

        # Inicjalizuj sÅ‚ownik nieregularnych form
        self.irregular_dict = IrregularFormsDict()

    def analyze(self, text: str) -> List[Dict[str, Any]]:
        """Analiza morfologiczna z fallback."""
        if not self.available:
            return self._fallback_analysis(text)

        try:
            analysis = self.morfeusz.analyse(text)
            results = []

            for segment in analysis:
                start_idx, end_idx, interpretation = segment
                form, lemma, tag, labels, qualifiers = interpretation

                results.append({
                    'form': form,
                    'lemma': lemma,
                    'tag': tag,
                    'labels': labels if labels else [],
                    'qualifiers': qualifiers if qualifiers else [],
                    'start': start_idx,
                    'end': end_idx,
                    'source': 'morfeusz2'
                })

            return results
        except Exception as e:
            logger.warning(f"Morfeusz bÅ‚Ä…d: {e}, uÅ¼ywam fallback")
            return self._fallback_analysis(text)

    def _fallback_analysis(self, text: str) -> List[Dict[str, Any]]:
        """Prosta analiza morfologiczna fallback dla polskiego."""
        words = text.split()
        results = []

        for i, word in enumerate(words):
            # Prosta heurystyka morfologiczna dla polskiego
            lemma = self._guess_lemma(word)
            tag = self._guess_morphological_tag(word)

            results.append({
                'form': word,
                'lemma': lemma,
                'tag': tag,
                'labels': [],
                'qualifiers': [],
                'start': i,
                'end': i + 1,
                'source': 'fallback'
            })

        return results

    def _guess_lemma(self, word: str) -> str:
        """Prosta heurystyka lematyzacji z obsÅ‚ugÄ… nieregularnych form."""
        word_lower = word.lower()

        # NAJPIERW sprawdÅº sÅ‚ownik nieregularnych form
        irregular = self.irregular_dict.lookup(word_lower)
        if irregular:
            return irregular['lemma']

        # Usuwanie prostych koÅ„cÃ³wek fleksyjnych
        if word_lower.endswith(('em', 'ie', 'Ä…', 'y', 'e')):
            return word_lower[:-1]
        elif word_lower.endswith(('ami', 'ach', 'Ã³w')):
            return word_lower[:-2]
        else:
            return word_lower

    def _guess_morphological_tag(self, word: str) -> str:
        """Rozszerzona heurystyka tagÃ³w morfologicznych."""
        word_lower = word.lower()

        # Czasowniki
        if word_lower.endswith(('aÄ‡', 'eÄ‡', 'iÄ‡', 'owaÄ‡', 'nÄ…Ä‡')):
            return 'verb:inf'
        elif word_lower.endswith(('Ä™', 'esz', 'e', 'emy', 'ecie', 'Ä…')):
            return 'verb:fin:sg:3:pres'

        # Rzeczowniki
        elif word_lower.endswith(('oÅ›Ä‡', 'enie', 'anie', 'cie')):
            return 'subst:sg:nom:f'
        elif word_lower.endswith(('ek', 'ik', 'ar', 'arz')):
            return 'subst:sg:nom:m1'
        elif word_lower.endswith(('a', 'ka', 'ia')):
            return 'subst:sg:nom:f'
        elif word_lower.endswith(('o', 'um', 'e')):
            return 'subst:sg:nom:n'

        # Przymiotniki
        elif word_lower.endswith(('ny', 'ty', 'Å‚y', 'wy', 'owy', 'owy')):
            return 'adj:sg:nom:m1:pos'
        elif word_lower.endswith(('na', 'ta', 'Å‚a', 'wa', 'owa')):
            return 'adj:sg:nom:f:pos'
        elif word_lower.endswith(('ne', 'te', 'Å‚e', 'we', 'owe')):
            return 'adj:sg:nom:n:pos'

        # PrzysÅ‚Ã³wki
        elif word_lower.endswith(('sko', 'Å›nie', 'Ä…co', 'nie')):
            return 'adv:pos'

        # SpÃ³jniki i partykuÅ‚y
        elif word_lower in ['i', 'a', 'ale', 'Å¼e', 'bo', 'oraz', 'lub']:
            return 'conj'
        elif word_lower in ['nie', 'tak', 'juÅ¼', 'jeszcze', 'bardzo']:
            return 'part'

        # Przyimki
        elif word_lower in ['w', 'na', 'do', 'z', 'o', 'od', 'po', 'za', 'przed']:
            return 'prep:inst'

        # DomyÅ›lnie rzeczownik
        else:
            return 'subst:sg:nom:m3'

    def _detect_gender(self, word: str, tag: str) -> str:
        """Rozpoznaj rodzaj gramatyczny sÅ‚owa (m1/m2/m3/f/n)."""
        word_lower = word.lower()

        # m1 (mÄ™skoosobowy) - osoby pÅ‚ci mÄ™skiej
        m1_endings = ['ek', 'ik', 'arz', 'cz', 'nik', 'ak', 'ec']
        m1_words = ['czÅ‚owiek', 'mÄ™Å¼czyzna', 'chÅ‚opiec', 'ojciec', 'syn', 'brat',
                    'dziadek', 'wuj', 'kuzyn', 'kolega', 'przyjaciel', 'nauczyciel']

        if word_lower in m1_words:
            return 'm1'
        for ending in m1_endings:
            if word_lower.endswith(ending) and len(word_lower) > len(ending) + 2:
                # Heurystyka: jeÅ›li ma typowÄ… koÅ„cÃ³wkÄ™ mÄ™skoosobowÄ…
                return 'm1'

        # f (Å¼eÅ„ski)
        f_endings = ['a', 'ka', 'ia', 'oÅ›Ä‡', 'oÅ›Ä‡', 'ja', 'ni', 'yni', 'owa', 'wa']
        f_words = ['kobieta', 'matka', 'cÃ³rka', 'siostra', 'babcia', 'ciocia']

        if word_lower in f_words:
            return 'f'
        for ending in f_endings:
            if word_lower.endswith(ending):
                return 'f'

        # n (nijaki)
        n_endings = ['o', 'um', 'Ä™', 'cie', 'nie', 'dÅ‚o', 'tko']
        n_words = ['dziecko', 'pole', 'morze', 'okno', 'miasto']

        if word_lower in n_words:
            return 'n'
        for ending in n_endings:
            if word_lower.endswith(ending):
                return 'n'

        # m2 (mÄ™ski Å¼ywotny) - zwierzÄ™ta
        m2_words = ['pies', 'kot', 'koÅ„', 'ptak', 'sÅ‚oÅ„', 'lew', 'wilk']
        if word_lower in m2_words:
            return 'm2'

        # m3 (mÄ™ski nieÅ¼ywotny) - rzeczy, domyÅ›lny dla rzeczownikÃ³w mÄ™skich
        return 'm3'

    def _detect_number(self, word: str) -> str:
        """Rozpoznaj liczbÄ™ gramatycznÄ… (sg/pl)."""
        word_lower = word.lower()

        # Liczba mnoga - koÅ„cÃ³wki
        pl_endings = [
            'y', 'i',           # nom.pl: domy, konie
            'Ã³w', 'i',          # gen.pl: domÃ³w, koni
            'om',               # dat.pl: domom
            'ami', 'mi',        # inst.pl: domami
            'ach',              # loc.pl: domach
        ]

        # WyjÄ…tki liczby mnogiej
        pl_words = ['ludzie', 'dzieci', 'oczy', 'rÄ™ce', 'nogi']
        if word_lower in pl_words:
            return 'pl'

        # SprawdÅº koÅ„cÃ³wki liczby mnogiej
        for ending in pl_endings:
            if word_lower.endswith(ending) and len(word_lower) > len(ending) + 2:
                # Dodatkowa heurystyka: unikaj faÅ‚szywych trafieÅ„ dla krÃ³tkich sÅ‚Ã³w
                if ending in ['y', 'i'] and len(word_lower) > 3:
                    return 'pl'
                elif ending not in ['y', 'i']:
                    return 'pl'

        # DomyÅ›lnie liczba pojedyncza
        return 'sg'

    def _detect_verb_aspect(self, lemma: str) -> str:
        """Rozpoznaj aspekt czasownika (perf/imperf) z obsÅ‚ugÄ… sÅ‚ownika nieregularnych."""
        lemma_lower = lemma.lower()

        # NAJPIERW sprawdÅº sÅ‚ownik nieregularnych form
        irregular = self.irregular_dict.lookup(lemma_lower)
        if irregular:
            return irregular['aspect']

        # Prefiksy dokonane (perfektywne)
        perfective_prefixes = [
            'z', 'wy', 'prze', 'roz', 'po', 'na', 'u', 's', 'do',
            'od', 'przy', 'we', 'za', 'ob'
        ]

        for prefix in perfective_prefixes:
            if lemma_lower.startswith(prefix) and len(lemma_lower) > len(prefix) + 2:
                # SprawdÅº czy to faktycznie prefiks a nie czÄ™Å›Ä‡ rdzenia
                if lemma_lower.startswith(prefix + 'r') or lemma_lower.startswith(prefix + 'p'):
                    return 'perf'

        # Sufiksy niedokonane (imperfektywne)
        imperfective_suffixes = ['ywaÄ‡', 'iwaÄ‡', 'owaÄ‡']
        for suffix in imperfective_suffixes:
            if lemma_lower.endswith(suffix):
                return 'imperf'

        # Typowe czasowniki dokonane i niedokonane (sÅ‚ownik)
        perfective_verbs = ['zrobiÄ‡', 'powiedzieÄ‡', 'wziÄ…Ä‡', 'daÄ‡', 'kupiÄ‡', 'sprzedaÄ‡']
        imperfective_verbs = ['robiÄ‡', 'mÃ³wiÄ‡', 'braÄ‡', 'dawaÄ‡', 'kupowaÄ‡', 'sprzedawaÄ‡']

        if lemma_lower in perfective_verbs:
            return 'perf'
        elif lemma_lower in imperfective_verbs:
            return 'imperf'

        # DomyÅ›lnie niedokonany
        return 'imperf'

    def _detect_verb_tense(self, word: str) -> str:
        """Rozpoznaj czas czasownika (pres/past/fut)."""
        word_lower = word.lower()

        # Czas przeszÅ‚y - koÅ„cÃ³wki
        past_endings = ['Å‚em', 'Å‚eÅ›', 'Å‚', 'Å‚a', 'Å‚o', 'liÅ›my', 'Å‚yÅ›my', 'li', 'Å‚y']
        for ending in past_endings:
            if word_lower.endswith(ending):
                return 'past'

        # Czas przyszÅ‚y - formy analityczne z 'bÄ™dÄ™'
        future_forms = ['bÄ™dÄ™', 'bÄ™dziesz', 'bÄ™dzie', 'bÄ™dziemy', 'bÄ™dziecie', 'bÄ™dÄ…']
        if word_lower in future_forms:
            return 'fut'

        # Czas teraÅºniejszy - koÅ„cÃ³wki (domyÅ›lny)
        present_endings = ['Ä™', 'esz', 'e', 'emy', 'ecie', 'Ä…', 'am', 'asz', 'amy', 'acie', 'ajÄ…']
        for ending in present_endings:
            if word_lower.endswith(ending):
                return 'pres'

        # DomyÅ›lnie teraÅºniejszy
        return 'pres'

    def _detect_adj_degree(self, word: str) -> str:
        """Rozpoznaj stopieÅ„ przymiotnika (pos/com/sup)."""
        word_lower = word.lower()

        # StopieÅ„ najwyÅ¼szy - prefiks 'naj-'
        if word_lower.startswith('naj'):
            return 'sup'

        # StopieÅ„ wyÅ¼szy - koÅ„cÃ³wki
        comparative_endings = ['szy', 'ejszy', 'iejszy']
        for ending in comparative_endings:
            if word_lower.endswith(ending):
                return 'com'

        # StopieÅ„ rÃ³wny (domyÅ›lny)
        return 'pos'

class SpacyAnalyzer:
    """spaCy analyzer for Polish syntactic analysis with fallback."""

    def __init__(self):
         # Check global installation status
        if installation_status.get('spacy'):
            try:
                import spacy
                self.nlp = spacy.load("pl_core_news_lg") # Try lg first
                self.available = True
                logger.info("âœ… spaCy Polish model loaded")
            except:
                 try:
                     self.nlp = spacy.load("pl_core_news_md") # Try md
                     self.available = True
                     logger.info("âœ… spaCy Polish model loaded (md)")
                 except:
                     try:
                         self.nlp = spacy.load("pl_core_news_sm") # Try sm
                         self.available = True
                         logger.info("âœ… spaCy Polish model loaded (sm)")
                     except Exception as e:
                         self.nlp = None
                         self.available = False
                         logger.warning(f"âŒ spaCy not available: {e}")
        else:
            self.nlp = None
            self.available = False
            logger.info("â„¹ï¸ spaCy niedostÄ™pny - uÅ¼ywam fallback")


    def analyze(self, text: str) -> Optional[Any]:
        """Analyze text with spaCy or fallback."""
        if not self.available:
            return self._fallback_analysis(text)

        try:
            return self.nlp(text)
        except Exception as e:
            logger.warning(f"spaCy bÅ‚Ä…d: {e}, uÅ¼ywam fallback")
            return self._fallback_analysis(text)

    def _fallback_analysis(self, text: str) -> Optional[Any]:
        """Prosta analiza fallback spaCy (syntaktyka)."""
        # Create a dummy Doc-like object for fallback
        class DummyMorph:
            """Dummy class to mimic spaCy Morph object with .get method returning a list."""
            def __init__(self, data: Dict = None):
                self._data = data if data is not None else {}

            def get(self, key, default=None):
                # Return a list or default, mimicking spaCy's behavior
                return [self._data[key]] if key in self._data else default


        class DummyToken:
            def __init__(self, text, pos, dep, lemma, morph_data: Dict = None, is_stop=False, head=None, children=None):
                self.text = text
                self.pos_ = pos
                self.dep_ = dep
                self.lemma_ = lemma
                # Initialize morph as a DummyMorph object
                self.morph = DummyMorph(morph_data)
                self.is_stop = is_stop
                self.head = head if head is not None else self
                self.children = children if children is not None else []
                self.has_vector = False
                self.vector = None

            def __str__(self):
                return self.text

            @property
            def ancestors(self):
                # Simplified: assume max depth 2 for fallback
                return [self.head] if self.head != self else []


        class DummyDoc:
            def __init__(self, text):
                self.text = text
                words = text.split()
                self.tokens = []
                root_found = False
                for i, word in enumerate(words):
                    # Simple heuristic for POS and dependency
                    pos = 'X'
                    dep = 'dep'
                    lemma = word.lower()
                    morph_data = {} # Default empty morph data

                    if word.lower() in ['jest', 'sÄ…', 'byÄ‡']:
                        pos = 'VERB'
                        dep = 'ROOT'
                        root_found = True
                        # Add dummy morph data for verb
                        morph_data = {"Aspect": ["Imp"], "Mood": ["Ind"]}
                    elif word.lower() in ['i', 'a', 'ale']:
                         pos = 'CCONJ'
                    elif word.lower() in ['.', ',', '!', '?']:
                         pos = 'PUNCT'

                    # Pass dummy morph data
                    self.tokens.append(DummyToken(word, pos, dep, lemma, morph_data=morph_data))

                # Basic dependency linkage for fallback
                if root_found:
                    root_token = next((t for t in self.tokens if t.dep_ == 'ROOT'), None)
                    if root_token:
                        for token in self.tokens:
                            if token != root_token:
                                token.head = root_token # Link all others to root


            def __iter__(self):
                return iter(self.tokens)

            def __len__(self):
                return len(self.tokens)

        logger.info("Using spaCy fallback analysis.")
        return DummyDoc(text)

class StanzaAnalyzer:
    """Stanza analyzer for Polish with fallback."""

    def __init__(self):
         # Check global installation status
        if installation_status.get('stanza'):
            try:
                import stanza
                # Download model if needed (handled in install_dependencies)
                self.nlp = stanza.Pipeline('pl',
                                        processors='tokenize,pos,lemma,depparse',
                                        verbose=False,
                                        use_gpu=False)
                self.available = True
                logger.info("âœ… Stanza Polish pipeline loaded")
            except Exception as e:
                self.nlp = None
                self.available = False
                logger.warning(f"âŒ Stanza not available: {e}")
        else:
            self.nlp = None
            self.available = False
            logger.info("â„¹ï¸ Stanza niedostÄ™pny - uÅ¼ywam fallback")


    def analyze(self, text: str) -> Optional[Any]:
        """Analyze text with Stanza or fallback."""
        if not self.available:
            return self._fallback_analysis(text)

        try:
            doc = self.nlp(text)
            return doc
        except Exception as e:
            logger.warning(f"Stanza bÅ‚Ä…d: {e}, uÅ¼ywam fallback")
            return self._fallback_analysis(text)

    def _fallback_analysis(self, text: str) -> Optional[Any]:
        """Prosta analiza fallback Stanza."""
        logger.info("Using Stanza fallback analysis.")
        # Return a simple structure, possibly mirroring the Stanza Doc structure partially
        class DummyStanzaWord:
            def __init__(self, text, lemma, upos, xpos, feats, head, deprel):
                self.text = text
                self.lemma = lemma
                self.upos = upos
                self.xpos = xpos
                self.feats = feats
                self.head = head
                self.deprel = deprel

        class DummyStanzaSentence:
            def __init__(self, text):
                self.text = text
                words = text.split()
                self.words = []
                for word in words:
                    # Simple heuristic
                    lemma = word.lower()
                    upos = 'X'
                    xpos = '_'
                    feats = '_'
                    head = 0 # Simplified head (0 for root)
                    deprel = 'root' if head == 0 else 'dep'
                    self.words.append(DummyStanzaWord(word, lemma, upos, xpos, feats, head, deprel))

        class DummyStanzaDoc:
            def __init__(self, text):
                self.text = text
                self.sentences = [DummyStanzaSentence(text)] # One sentence for simplicity

        return DummyStanzaDoc(text)


# ============================================================================
# GÅÃ“WNY PROCESOR GTMÃ˜
# ============================================================================

class GTMOProcessor:
    """GÅ‚Ã³wny procesor analizy GTMÃ˜."""

    def __init__(self):
        print("ðŸ”§ InicjalizujÄ™ procesor GTMÃ˜...")
        self.morfeusz = MorfeuszAnalyzer()
        self.spacy = SpacyAnalyzer()
        self.stanza = StanzaAnalyzer() # Added Stanza initialization
        self.derivational = DerivationalAnalyzer()  # Analiza derywacyjna
        self.collocation = CollocationAnalyzer()  # Analiza kolokacji
        self.semantic = SemanticEmbeddingAnalyzer(use_pretrained=False)  # Analiza semantyczna
        self._init_mappings()
        print("âœ… Procesor GTMÃ˜ gotowy")

    def _init_mappings(self):
        """Inicjalizacja mapowaÅ„ morfologicznych."""
        self.case_map = {case.tag: case for case in PolishCase}
        # Mapowanie POS tagÃ³w spaCy/Stanza na enum PolishPOS
        self.pos_map = {
            'NOUN': PolishPOS.SUBST,
            'ADJ': PolishPOS.ADJ,
            'VERB': PolishPOS.VERB,
            'ADV': PolishPOS.ADV,
            'NUM': PolishPOS.NUM,
            'PRON': PolishPOS.PRON,
            'ADP': PolishPOS.PREP,
            'CCONJ': PolishPOS.CONJ,
            'SCONJ': PolishPOS.CONJ,
            'PART': PolishPOS.PART,
            'PUNCT': PolishPOS.INTERP,
            'DET': PolishPOS.PRON, # Mapping DET to PRON as it's not in PolishPOS enum
            'X': PolishPOS.PART # Mapping X to PART as a general fallback
        }


    def calculate_coordinates(self, text: str) -> Tuple[GTMOCoordinates, Configuration, Dict]:
        """Oblicz wspÃ³Å‚rzÄ™dne GTMÃ˜ dla tekstu."""
        config = Configuration()
        metadata = {'text': text, 'analyses': [], 'morphological_features': []}

        # Ternary Stream 1: Morphological analysis (Morfeusz or fallback)
        morfeusz_analysis = self.morfeusz.analyze(text)
        morph_coords = self._process_morphology(morfeusz_analysis, metadata)

        # Ternary Stream 2: Syntactic analysis (spaCy or fallback)
        spacy_doc = self.spacy.analyze(text)
        syntax_coords, config = self._process_syntax(spacy_doc, metadata) # Pass spacy doc

        # Ternary Stream 3: Semantic computation (Simplified)
        semantic_coords = self._compute_semantics(text)

        # Combine three streams with weighted average
        weights = [0.4, 0.35, 0.25]  # Morphology, Syntax, Semantics
        combined = np.average(
            [morph_coords.to_array(), syntax_coords.to_array(), semantic_coords.to_array()],
            weights=weights,
            axis=0
        )

        coords = GTMOCoordinates(*combined)

        # NOWE: Analiza derywacyjna (wpÅ‚yw prefiksÃ³w/sufiksÃ³w na GTMÃ˜)
        derivational_mods = {'determination': 0.0, 'stability': 0.0, 'entropy': 0.0}
        metadata['derivational_features'] = []

        for analysis in morfeusz_analysis:
            word = analysis.get('form', '')
            deriv_result = self.derivational.analyze_word(word)

            if deriv_result['has_prefix'] or deriv_result['has_suffix']:
                metadata['derivational_features'].append({
                    'word': word,
                    'prefix': deriv_result['prefix'],
                    'suffix': deriv_result['suffix'],
                    'stem': deriv_result['stem']
                })

                # Akumuluj modyfikacje GTMÃ˜
                for coord, value in deriv_result['gtmo_modifications'].items():
                    derivational_mods[coord] += value

        # Zastosuj modyfikacje derywacyjne (normalizuj przez liczbÄ™ sÅ‚Ã³w)
        if len(morfeusz_analysis) > 0:
            coords.determination = np.clip(
                coords.determination + derivational_mods['determination'] / len(morfeusz_analysis),
                0, 1
            )
            coords.stability = np.clip(
                coords.stability + derivational_mods['stability'] / len(morfeusz_analysis),
                0, 1
            )
            coords.entropy = np.clip(
                coords.entropy + derivational_mods['entropy'] / len(morfeusz_analysis),
                0, 1
            )

        # NOWE: Analiza kolokacji (wpÅ‚yw idiomÃ³w i frazeologizmÃ³w)
        found_collocations = self.collocation.find_collocations(text)
        metadata['collocations'] = found_collocations

        if found_collocations:
            coll_effect = self.collocation.calculate_collocation_effect(found_collocations)
            # Kolokacje majÄ… silny wpÅ‚yw, wiÄ™c uÅ¼ywamy wiÄ™kszej wagi
            coords.determination = np.clip(
                coords.determination * 0.6 + coll_effect['determination'] * 0.4,
                0, 1
            )
            coords.stability = np.clip(
                coords.stability * 0.6 + coll_effect['stability'] * 0.4,
                0, 1
            )
            coords.entropy = np.clip(
                coords.entropy * 0.6 + coll_effect['entropy'] * 0.4,
                0, 1
            )

        # config.position and config.scale set in _process_syntax
        config.time = datetime.now().timestamp()

        # Classify to nearest attractor
        metadata['attractor'] = self._find_nearest_attractor(coords)

        return coords, config, metadata

    def _process_morphology(self, morfeusz_analysis: List[Dict], metadata: Dict) -> GTMOCoordinates:
        """Przetwarzanie cech morfologicznych z rozszerzonÄ… analizÄ…."""
        coords_list = []

        for analysis in morfeusz_analysis:
            tag_parts = analysis.get('tag', '').split(':')
            tag = analysis.get('tag', '')
            form = analysis.get('form', '')
            lemma = analysis.get('lemma', '')

            # ISTNIEJÄ„CE: WyciÄ…gnij przypadek
            for tag_part in tag_parts:
                if tag_part in self.case_map:
                    case = self.case_map[tag_part]
                    coords_list.append(case.coords.to_array())
                    metadata['analyses'].append({
                        'type': 'case',
                        'value': case.polish_name,
                        'source': analysis.get('source', 'morfeusz2' if self.morfeusz.available else 'fallback')
                    })
                    break

            # NOWE: Wykryj i dodaj rodzaj gramatyczny
            if 'subst' in tag or 'adj' in tag:
                gender = self.morfeusz._detect_gender(form, tag)
                if gender in GENDER_COORDS:
                    coords_list.append(np.array(GENDER_COORDS[gender]))
                    metadata['morphological_features'].append({
                        'type': 'gender',
                        'value': gender,
                        'form': form
                    })

            # NOWE: Wykryj i dodaj liczbÄ™
            number = self.morfeusz._detect_number(form)
            if number in NUMBER_COORDS:
                coords_list.append(np.array(NUMBER_COORDS[number]))
                metadata['morphological_features'].append({
                    'type': 'number',
                    'value': number,
                    'form': form
                })

            # NOWE: Dla czasownikÃ³w - aspekt i czas
            if 'verb' in tag:
                # Aspekt
                aspect = self.morfeusz._detect_verb_aspect(lemma)
                if aspect in ASPECT_COORDS:
                    coords_list.append(np.array(ASPECT_COORDS[aspect]))
                    metadata['morphological_features'].append({
                        'type': 'aspect',
                        'value': aspect,
                        'lemma': lemma
                    })

                # Czas
                tense = self.morfeusz._detect_verb_tense(form)
                if tense in TENSE_COORDS:
                    coords_list.append(np.array(TENSE_COORDS[tense]))
                    metadata['morphological_features'].append({
                        'type': 'tense',
                        'value': tense,
                        'form': form
                    })

            # NOWE: Dla przymiotnikÃ³w - stopieÅ„
            if 'adj' in tag:
                degree = self.morfeusz._detect_adj_degree(form)
                if degree in DEGREE_COORDS:
                    coords_list.append(np.array(DEGREE_COORDS[degree]))
                    metadata['morphological_features'].append({
                        'type': 'degree',
                        'value': degree,
                        'form': form
                    })

        if coords_list:
            return GTMOCoordinates(*np.mean(coords_list, axis=0))
        return GTMOCoordinates(0.5, 0.5, 0.5)

    def _process_syntax(self, spacy_doc: Optional[Any], metadata: Dict) -> Tuple[GTMOCoordinates, Configuration]:
        """Przetwarzanie cech syntaktycznych."""
        coords_list = []
        config = Configuration() # Initialize config here

        if spacy_doc:
            try:
                # Analyze dependency tree structure
                dependency_scores = {
                    'ROOT': (0.9, 0.8, 0.1),      # Root verb - high determination
                    'nsubj': (0.8, 0.7, 0.2),     # Nominal subject
                    'obj': (0.7, 0.6, 0.3),       # Direct object
                    'iobj': (0.6, 0.5, 0.4),      # Indirect object
                    'ccomp': (0.5, 0.4, 0.5),     # Clausal complement
                    'xcomp': (0.5, 0.4, 0.5),     # Open clausal complement
                    'advcl': (0.4, 0.3, 0.6),     # Adverbial clause
                    'advmod': (0.3, 0.4, 0.6),    # Adverbial modifier
                    'amod': (0.6, 0.5, 0.4),      # Adjectival modifier
                    'nmod': (0.5, 0.6, 0.4),      # Nominal modifier
                    'det': (0.8, 0.8, 0.2),       # Determiner
                    'case': (0.7, 0.7, 0.3),      # Case marking
                    'punct': (0.9, 0.9, 0.1),     # Punctuation
                    'conj': (0.5, 0.5, 0.5),      # Conjunction
                    'cc': (0.5, 0.5, 0.5),        # Coordinating conjunction
                    'aux': (0.6, 0.6, 0.4),       # Auxiliary
                    'cop': (0.7, 0.7, 0.3),       # Copula
                    'mark': (0.4, 0.4, 0.6),      # Marker
                    'appos': (0.5, 0.4, 0.5),     # Appositional modifier
                    'acl': (0.4, 0.3, 0.6),       # Adjectival clause
                    'dep': (0.3, 0.3, 0.7)        # Unspecified dependency
                }

                root_count = 0
                total_depth = 0
                max_depth = 0

                for token in spacy_doc:
                    # Map POS tags
                    if token.pos_ in self.pos_map:
                        pos_enum = self.pos_map[token.pos_]
                        coords_list.append(pos_enum.coords.to_array())
                        metadata['analyses'].append({
                            'type': 'pos',
                            'value': pos_enum.polish_name, # Correctly access polish_name from enum member
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })
                    elif token.pos_ != 'SPACE': # Ignore whitespace tokens
                         metadata['analyses'].append({
                            'type': 'pos',
                            'value': token.pos_, # Use raw POS if not in map
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })


                    # Analyze dependency relations
                    if token.dep_ in dependency_scores:
                        dep_coords = dependency_scores[token.dep_]
                        coords_list.append(np.array(dep_coords))
                        metadata['analyses'].append({
                            'type': 'dependency',
                            'value': token.dep_,
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })
                    elif token.dep_ != 'punct': # Ignore punctuation dependencies
                         metadata['analyses'].append({
                            'type': 'dependency',
                            'value': token.dep_,
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })


                    # Calculate syntactic tree depth
                    # Check if token has head before accessing ancestors
                    if token.head != token:
                        ancestors = list(token.ancestors)
                        depth = len(ancestors)
                        total_depth += depth
                        max_depth = max(max_depth, depth)

                    # Special handling for ROOT
                    if token.dep_ == "ROOT":
                        root_count += 1
                        config.position[0] = 0.8  # High semantic centrality

                        # Analyze verb properties for Polish
                        if token.pos_ == "VERB":
                            # Safely access morph attributes and handle lists
                            aspect_data = None
                            mood_data = None
                            if hasattr(token, 'morph') and token.morph is not None:
                                 # Use .get() to retrieve the list from the morph object
                                 if hasattr(token.morph, 'get'): # spaCy Morph or similar with .get
                                     aspect_data = token.morph.get("Aspect")
                                     mood_data = token.morph.get("Mood")
                                 elif isinstance(token.morph, dict): # Fallback case
                                     aspect_data = token.morph.get("Aspect")
                                     mood_data = token.morph.get("Mood")


                            if aspect_data and isinstance(aspect_data, list) and len(aspect_data) > 0: # Check if list and not empty
                                aspect = str(aspect_data[0]) # Access element by index
                                if aspect == "Perf":
                                    config.position[1] = 0.7  # Perfective - more determined
                                else:
                                    config.position[1] = 0.4  # Imperfective - less determined

                            if mood_data and isinstance(mood_data, list) and len(mood_data) > 0: # Check if list and not empty
                                mood = str(mood_data[0]) # Access element by index
                                mood_values = {
                                    "Ind": 0.8,  # Indicative - factual
                                    "Imp": 0.6,  # Imperative - directive
                                    "Cnd": 0.3   # Conditional - hypothetical
                                }
                                config.position[2] = mood_values.get(mood, 0.5)


                    # Analyze Polish-specific features (integrated into the main loop)
                    if token.pos_ == "ADP":  # Preposition
                        # Different prepositions require different cases
                        prep_case_patterns = {
                            'w': (0.6, 0.7, 0.3),    # w + locative
                            'na': (0.7, 0.6, 0.3),   # na + accusative/locative
                            'z': (0.5, 0.5, 0.5),    # z + genitive/instrumental
                            'do': (0.8, 0.7, 0.2),   # do + genitive
                            'od': (0.7, 0.6, 0.3),   # od + genitive
                            'przez': (0.6, 0.5, 0.4), # przez + accusative
                            'dla': (0.6, 0.6, 0.4),  # dla + genitive
                            'o': (0.5, 0.4, 0.5),    # o + locative/accusative
                            'po': (0.4, 0.3, 0.6),   # po + locative/accusative
                            'za': (0.5, 0.5, 0.5)    # za + instrumental/accusative
                        }
                        if token.text.lower() in prep_case_patterns:
                            coords_list.append(np.array(prep_case_patterns[token.text.lower()]))


                # Calculate average tree depth for abstraction scale
                avg_depth = total_depth / len(spacy_doc) if len(spacy_doc) > 0 else 1
                config.scale = min(1.0, avg_depth / 5.0)

                # Analyze sentence complexity
                if len(spacy_doc) > 0:
                    # Simple sentence vs complex sentence
                    if root_count == 1 and max_depth <= 2:
                        # Simple sentence - higher stability
                        coords_list.append(np.array([0.7, 0.8, 0.2]))
                    elif root_count > 1 or max_depth > 4:
                        # Complex sentence - higher entropy
                        coords_list.append(np.array([0.4, 0.3, 0.7]))
                    else:
                        # Medium complexity
                        coords_list.append(np.array([0.5, 0.5, 0.5]))


            except Exception as e:
                logger.warning(f"BÅ‚Ä…d przetwarzania spaCy: {e}")

        if coords_list:
            coords = GTMOCoordinates(*np.mean(coords_list, axis=0))
        else:
            coords = GTMOCoordinates(0.5, 0.5, 0.5)

        return coords, config

    def _compute_semantics(self, text: str) -> GTMOCoordinates:
        """Oblicz cechy semantyczne z uÅ¼yciem klastrÃ³w semantycznych."""
        text_lower = text.lower()
        words = text_lower.split()

        determination = 0.5
        stability = 0.5
        entropy = 0.5

        # Count sentence type indicators
        if '?' in text:
            entropy += 0.3
            stability -= 0.2
            determination -= 0.1
        elif '!' in text:
            determination += 0.1
            entropy += 0.1
        elif '.' in text:
            stability += 0.1
            determination += 0.05

        # NOWE: UÅ¼yj analizy semantycznej z embeddings/klastrÃ³w
        semantic_context = self.semantic.calculate_semantic_context(words)

        # PoÅ‚Ä…cz podstawowÄ… analizÄ™ z kontekstem semantycznym (waga 0.4 dla semantyki)
        determination = determination * 0.6 + semantic_context['determination'] * 0.4
        stability = stability * 0.6 + semantic_context['stability'] * 0.4
        entropy = entropy * 0.6 + semantic_context['entropy'] * 0.4

        # Simple sentiment check (very basic) - jako dodatek
        positive_words = ['dobry', 'miÅ‚y', 'szczÄ™Å›cie', 'radoÅ›Ä‡', 'piÄ™kny']
        negative_words = ['zÅ‚y', 'smutny', 'bÃ³l', 'strach', 'Å›mierÄ‡']

        sentiment_score = 0
        for word in words:
            if word in positive_words:
                sentiment_score += 1
            elif word in negative_words:
                sentiment_score -= 1

        if sentiment_score > 0:
            determination += 0.05  # Positive sentiment adds some clarity
        elif sentiment_score < 0:
            entropy += 0.05  # Negative sentiment might add complexity

        # Clip to valid range
        coords = GTMOCoordinates(
            np.clip(determination, 0, 1),
            np.clip(stability, 0, 1),
            np.clip(entropy, 0, 1)
        )

        return coords


    def _find_nearest_attractor(self, coords: GTMOCoordinates) -> Dict:
        """ZnajdÅº najbliÅ¼szy atraktor wiedzy."""
        min_distance = float('inf')
        nearest = None

        for attractor in KnowledgeAttractor:
            distance = coords.distance_to(attractor.coords)
            if distance < min_distance:
                min_distance = distance
                nearest = attractor

        return {
            'symbol': nearest.symbol,
            'name': nearest.display_name,
            'distance': float(min_distance),
            'coordinates': {
                'determination': nearest.coords.determination,
                'stability': nearest.coords.stability,
                'entropy': nearest.coords.entropy
            }
        }

# ============================================================================
# WIZUALIZATOR Z FALLBACK
# ============================================================================

class GTMOVisualizer:
    """Silnik wizualizacji wynikÃ³w analizy GTMÃ˜."""

    def __init__(self):
        self.available = True # Assume Plotly is available after installation attempt
        try:
            import plotly.graph_objects as go
            import plotly.express as px
            from plotly.subplots import make_subplots
        except ImportError:
            self.available = False
            logger.warning("Plotly niedostÄ™pny - wizualizacje wyÅ‚Ä…czone")


        self.attractor_colors = {
            'Î¨á´·': '#FF6B6B',  # Red
            'Î¨Ê°': '#4ECDC4',  # Cyan
            'Î¨á´º': '#45B7D1',  # Blue
            'Ã˜': '#96CEB4',   # Green
            'Î¨~': '#FECA57',  # Yellow
            'Î¨â—Š': '#DDA0DD'   # Plum
        }

    def create_configuration_space_plot(self, results: List[Dict]):
        """UtwÃ³rz interaktywnÄ… wizualizacjÄ™ 3D Przestrzeni Konfiguracyjnej."""
        if not self.available:
            print("âŒ Plotly niedostÄ™pny - nie moÅ¼na utworzyÄ‡ wizualizacji")
            return None

        try:
            fig = make_subplots(
                rows=2, cols=2,
                specs=[
                    [{'type': 'scatter3d', 'rowspan': 2}, {'type': 'scatter'}],
                    [None, {'type': 'scatter'}]
                ],
                subplot_titles=(
                    'PrzestrzeÅ„ Konfiguracyjna (3D)',
                    'Determinacja vs StabilnoÅ›Ä‡',
                    'RozkÅ‚ad Entropii'
                )
            )

            # WyciÄ…gnij dane
            sentences = [r['text'] for r in results]
            coords = [r['coordinates'] for r in results]
            attractors = [r['metadata']['attractor'] for r in results]

            # Wykres 3D scatter
            x = [c['determination'] for c in coords]
            y = [c['stability'] for c in coords]
            z = [c['entropy'] for c in coords]
            colors = [self.attractor_colors.get(a['symbol'], '#808080') for a in attractors]

            # Dodaj punkty zdaÅ„
            fig.add_trace(
                go.Scatter3d(
                    x=x, y=y, z=z,
                    mode='markers+text',
                    marker=dict(
                        size=10,
                        color=colors,
                        opacity=0.8,
                        line=dict(width=1, color='white')
                    ),
                    text=[f"{i+1}" for i in range(len(sentences))],
                    textposition='top center',
                    hovertemplate='<b>Zdanie %{text}</b><br>' +
                                  'Determinacja: %{x:.3f}<br>' +
                                  'StabilnoÅ›Ä‡: %{y:.3f}<br>' +
                                  'Entropia: %{z:.3f}<br>' +
                                  '<extra></extra>',
                    name='Zdania'
                ),
                row=1, col=1
            )

            # Dodaj atraktory
            for attractor in KnowledgeAttractor:
                fig.add_trace(
                    go.Scatter3d(
                        x=[attractor.coords.determination],
                        y=[attractor.coords.stability],
                        z=[attractor.coords.entropy],
                        mode='markers+text',
                        marker=dict(
                            size=20,
                            color=self.attractor_colors.get(attractor.symbol, '#808080'),
                            symbol='diamond',
                            opacity=1.0,
                            line=dict(width=2, color='black')
                        ),
                        text=[attractor.symbol],
                        textposition='top center',
                        name=attractor.display_name,
                        hovertemplate=f'<b>{attractor.display_name}</b><br>' +
                                      'D: %{x:.3f}<br>S: %{y:.3f}<br>E: %{z:.3f}<br>' +
                                      '<extra></extra>'
                    ),
                    row=1, col=1
                )

            # Projekcja 2D: Determinacja vs StabilnoÅ›Ä‡
            fig.add_trace(
                go.Scatter(
                    x=x, y=y,
                    mode='markers',
                    marker=dict(
                        size=8,
                        color=z,
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title='Entropia', x=1.15)
                    ),
                    text=[f"S{i+1}" for i in range(len(sentences))],
                    hovertemplate='<b>%{text}</b><br>' +
                                  'D: %{x:.3f}<br>S: %{y:.3f}<br>' +
                                  '<extra></extra>',
                    showlegend=False
                ),
                row=1, col=2
            )

            # RozkÅ‚ad entropii
            fig.add_trace(
                go.Histogram(
                    x=z,
                    nbinsx=20,
                    marker_color='#45B7D1',
                    opacity=0.7,
                    showlegend=False
                ),
                row=2, col=2
            )

            # Actual subplot axis titles based on make_subplots structure
            fig.update_layout(
                title={
                    'text': 'Analiza Przestrzeni Konfiguracyjnej GTMÃ˜',
                    'font': {'size': 20, 'color': '#2C3E50'}
                },
                showlegend=True,
                height=800,
                scene=dict(  # This is for the 3D plot (row 1, col 1)
                    xaxis_title='Determinacja â†’',
                    yaxis_title='StabilnoÅ›Ä‡ â†’',
                    zaxis_title='Entropia â†’',
                    camera=dict(
                        eye=dict(x=1.5, y=1.5, z=1.5)
                    )
                ),
                xaxis2_title='Determinacja', # Correct axis reference for row 1, col 2
                yaxis2_title='StabilnoÅ›Ä‡',   # Correct axis reference for row 1, col 2
                xaxis3=dict(title='Entropia'), # Correct way to set title for axis 3
                yaxis3=dict(title='Liczba')   # Correct way to set title for axis 3
            )

            return fig

        except Exception as e:
            logger.error(f"BÅ‚Ä…d tworzenia wizualizacji: {e}")
            return None

    def show_text_summary(self, results: List[Dict]):
        """PokaÅ¼ podsumowanie tekstowe gdy wizualizacje niedostÄ™pne."""
        print("\n" + "="*60)
        print("PODSUMOWANIE ANALIZY GTMÃ˜")
        print("="*60)

        for i, result in enumerate(results, 1):
            coords = result['coordinates']
            attractor = result['metadata']['attractor']

            print(f"\n{i}. '{result['text']}'")
            print(f"   ðŸ“Š WspÃ³Å‚rzÄ™dne: [{coords['determination']:.3f}, {coords['stability']:.3f}, {coords['entropy']:.3f}]")
            print(f"   ðŸŽ¯ Atraktor: {attractor['symbol']} ({attractor['name']})")
            print(f"   ðŸ“ Dystans: {attractor['distance']:.3f}")

        # Statystyki ogÃ³lne
        if results:
            avg_det = np.mean([r['coordinates']['determination'] for r in results])
            avg_stab = np.mean([r['coordinates']['stability'] for r in results])
            avg_ent = np.mean([r['coordinates']['entropy'] for r in results])

            print(f"\nðŸ“ˆ STATYSTYKI ÅšREDNIE:")
            print(f"   Determinacja: {avg_det:.3f}")
            print(f"   StabilnoÅ›Ä‡: {avg_stab:.3f}")
            print(f"   Entropia: {avg_ent:.3f}")
        else:
            print("\nðŸ“ˆ Brak wynikÃ³w do podsumowania.")

# ============================================================================
# GÅÃ“WNA KLASA ANALIZATORA
# ============================================================================

class GTMOAnalyzer:
    """GÅ‚Ã³wny analizator orkiestrujÄ…cy kompletny pipeline analizy GTMÃ˜."""
    def __init__(self):
        global installation_status

        logger.info("InicjalizujÄ™ Analizator GTMÃ˜...")

        # Check if installation status is available and indicates success
        # UÅ¼ywamy `globals()` do bezpiecznego sprawdzenia istnienia zmiennej
        if 'installation_status' not in globals() or not installation_status.get('morfeusz') or not installation_status.get('spacy') or not installation_status.get('stanza'):
            logger.warning("âš ï¸  ZaleÅ¼noÅ›ci nie zostaÅ‚y pomyÅ›lnie zainstalowane. Analiza moÅ¼e uÅ¼ywaÄ‡ fallbackÃ³w.")

            # PrÃ³ba ponownej instalacji
            installation_status = install_dependencies()

        self.processor = GTMOProcessor()
        self.visualizer = GTMOVisualizer()
        self.test_sentences = self._get_test_sentences()


    def _get_test_sentences(self) -> List[str]:
        """Pobierz 15 rÃ³Å¼norodnych polskich zdaÅ„ testowych."""
        return [
            # Fakty i pewnoÅ›ci
            "Warszawa jest stolicÄ… Polski.",
            "Dwa plus dwa rÃ³wna siÄ™ cztery.",

            # Pytania
            "Czy sprawiedliwoÅ›Ä‡ zawsze zwyciÄ™Å¼a?",
            "Kiedy nadejdzie prawdziwa wolnoÅ›Ä‡?",

            # NiepewnoÅ›Ä‡
            "MoÅ¼e jutro bÄ™dzie padaÄ‡ deszcz.",
            "Chyba nie zdÄ…Å¼ymy na pociÄ…g.",

            # PojÄ™cia abstrakcyjne
            "MiÅ‚oÅ›Ä‡ jest silniejsza niÅ¼ Å›mierÄ‡.",
            "SprawiedliwoÅ›Ä‡ powinna byÄ‡ Å›lepa.",

            # Paradoksy
            "To zdanie jest faÅ‚szywe.",
            "Wiem, Å¼e nic nie wiem.",

            # Polecenia
            "PrzyjdÅº jutro o Ã³smej rano!",
            "Nie zapomnij o spotkaniu.",

            # Emocjonalne
            "Och, jak piÄ™knie pachnÄ… te rÃ³Å¼e!",

            # ZÅ‚oÅ¼one
            "ChoÄ‡ burza szaleje, statek pÅ‚ynie dalej.",
            "Gdyby nie deszcz, poszlibyÅ›my na spacer."
        ]

    def analyze_all(self) -> List[Dict]:
        """Analizuj wszystkie zdania testowe."""
        results = []

        for i, sentence in enumerate(self.test_sentences, 1):
            logger.info(f"AnalizujÄ™ zdanie {i}/{len(self.test_sentences)}: {sentence[:50]}...")

            try:
                coords, config, metadata = self.processor.calculate_coordinates(sentence)

                result = {
                    'id': i,
                    'text': sentence,
                    'coordinates': {
                        'determination': float(coords.determination),
                        'stability': float(coords.stability),
                        'entropy': float(coords.entropy)
                    },
                    'configuration': config.to_dict(),
                    'metadata': metadata,
                    'timestamp': datetime.now().isoformat()
                }

                results.append(result)

            except Exception as e:
                logger.error(f"âŒ BÅ‚Ä…d podczas analizy zdania '{sentence[:50]}...': {e}")
                # Optionally append a failed analysis marker
                results.append({
                    'id': i,
                    'text': sentence,
                    'status': 'failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                continue


        return results

    def save_results(self, results: List[Dict]) -> str:
        """Zapisz wyniki do pliku JSON z timestamp."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'gtmo_analysis_{timestamp}.json'

        output = {
            'version': '2.1',
            'timestamp': datetime.now().isoformat(),
            'total_sentences': len([r for r in results if r.get('status') != 'failed']),
            'failed_sentences': len([r for r in results if r.get('status') == 'failed']),
            'installation_status': installation_status,
            'results': results,
            'statistics': self._calculate_statistics(results)
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"Wyniki zapisane do {filename}")
        return filename

    def _calculate_statistics(self, results: List[Dict]) -> Dict:
        """Oblicz statystyki podsumowujÄ…ce (tylko dla udanych analiz)."""
        successful_results = [r for r in results if r.get('status') != 'failed']

        if not successful_results:
             return {}

        coords = [r['coordinates'] for r in successful_results]

        return {
            'mean_determination': float(np.mean([c['determination'] for c in coords])),
            'mean_stability': float(np.mean([c['stability'] for c in coords])),
            'mean_entropy': float(np.mean([c['entropy'] for c in coords])),
            'std_determination': float(np.std([c['determination'] for c in coords])),
            'std_stability': float(np.std([c['stability'] for c in coords])),
            'std_entropy': float(np.std([c['entropy'] for c in coords])),
            'attractor_distribution': self._count_attractors(successful_results)
        }

    def _count_attractors(self, results: List[Dict]) -> Dict:
        """Policz rozkÅ‚ad najbliÅ¼szych atraktorÃ³w (tylko dla udanych analiz)."""
        attractors = {}
        for result in results:
            if 'metadata' in result and 'attractor' in result['metadata']:
                 symbol = result['metadata']['attractor']['symbol']
                 attractors[symbol] = attractors.get(symbol, 0) + 1
        return attractors


    def run_complete_analysis(self):
        """Uruchom kompletny pipeline analizy."""
        print("\n" + "="*60)
        print("ANALIZATOR GTMÃ˜ POLISH NLP")
        print("="*60 + "\n")

        # Analizuj zdania
        print("ðŸ” AnalizujÄ™ zdania...")
        results = self.analyze_all()

        # Zapisz wyniki
        if results:
            print("\nðŸ’¾ ZapisujÄ™ wyniki...")
            filename = self.save_results(results)
            print(f"âœ… Wyniki zapisane do: {filename}")

            # UtwÃ³rz wizualizacje (tylko dla udanych analiz)
            successful_results = [r for r in results if r.get('status') != 'failed']
            if successful_results:
                print("\nðŸ“Š GenerujÄ™ wizualizacje...")

                if self.visualizer.available:
                    fig1 = self.visualizer.create_configuration_space_plot(successful_results)

                    if fig1:
                        html_filename = 'gtmo_configuration_space.html'
                        fig1.write_html(html_filename)
                        print(f"âœ… Wizualizacja zapisana do {html_filename}")
                        fig1.show()
                    else:
                        print("âŒ BÅ‚Ä…d generowania wizualizacji")
                        self.visualizer.show_text_summary(successful_results)
                else:
                    print("â„¹ï¸ Plotly niedostÄ™pny - pokazujÄ™ podsumowanie tekstowe")
                    self.visualizer.show_text_summary(successful_results)
            else:
                 print("\nâ„¹ï¸ Brak udanych analiz do wizualizacji.")

        else:
             print("\nâŒ Brak wynikÃ³w analizy do zapisania lub wizualizacji.")


        print("\n" + "="*60)
        print("ANALIZA ZAKOÅƒCZONA")
        print("="*60)

        return results

# ============================================================================
# WYKONANIE
# ============================================================================

if __name__ == "__main__":
    # Inicjalizuj i uruchom analizator
    try:
        analyzer = GTMOAnalyzer()
        results = analyzer.run_complete_analysis()

        print(f"\nâœ… GTMÃ˜ Analiza zakoÅ„czona!")
        print(f"Przetworzono {len(results)} zdaÅ„ ({len([r for r in results if r.get('status') != 'failed'])} udanych)")
        print("SprawdÅº wygenerowane pliki dla szczegÃ³Å‚owych wynikÃ³w.")

    except Exception as e:
        print(f"\nâŒ Krytyczny bÅ‚Ä…d podczas inicjalizacji lub uruchamiania: {e}")
        import traceback
        traceback.print_exc()
