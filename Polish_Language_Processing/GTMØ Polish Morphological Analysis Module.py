#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GTMÃ˜ Polish NLP Implementation - NAPRAWIONA WERSJA
==================================================
Kompletna implementacja GTMÃ˜ dla jÄ™zyka polskiego z naprawionÄ… instalacjÄ…
optimized for Google Colab environment.

Author: GTMÃ˜ Research Team
Version: 2.1 - FIXED Installation Issues
Date: 2024
"""

import subprocess
import sys
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import json
import logging
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# NAPRAWIONA INSTALACJA ZALEÅ»NOÅšCI
# ============================================================================

def install_dependencies():
    """Naprawiona instalacja z lepszÄ… obsÅ‚ugÄ… bÅ‚Ä™dÃ³w."""
    print("ðŸ”§ Konfiguracja Å›rodowiska Google Colab...")

    # Lista podstawowych pakietÃ³w
    basic_packages = [
        'plotly',
        'kaleido',  # For plotly export
        'pandas',
        'numpy'
    ]

    # Zainstaluj podstawowe pakiety
    for package in basic_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"âœ… {package} juÅ¼ zainstalowany")
        except ImportError:
            print(f"ðŸ“¦ InstalujÄ™ {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])
            print(f"âœ… {package} zainstalowany")

    # SprÃ³buj zainstalowaÄ‡ Morfeusz2 z kilkoma metodami
    morfeusz_installed = install_morfeusz2()

    # Zainstaluj spaCy i model polski
    spacy_installed = install_spacy_polish()

    # Zainstaluj Stanza (opcjonalnie)
    stanza_installed = install_stanza_polish()

    print("\nðŸ“Š Podsumowanie instalacji:")
    print(f"   ðŸ“¦ Podstawowe pakiety: âœ…")
    print(f"   ðŸ”¤ Morfeusz2: {'âœ…' if morfeusz_installed else 'âŒ (bÄ™dzie uÅ¼ywany fallback)'}")
    print(f"   ðŸ§  spaCy: {'âœ…' if spacy_installed else 'âŒ (bÄ™dzie uÅ¼ywany fallback)'}")
    print(f"   ðŸ“š Stanza: {'âœ…' if stanza_installed else 'âŒ (bÄ™dzie uÅ¼ywany fallback)'}")

    return {
        'morfeusz': morfeusz_installed,
        'spacy': spacy_installed,
        'stanza': stanza_installed
    }

def install_morfeusz2():
    """Naprawiona instalacja Morfeusz2 z wieloma metodami."""
    print("ðŸ”¤ PrÃ³bujÄ™ zainstalowaÄ‡ Morfeusz2...")

    # Metoda 1: PrÃ³ba standardowej instalacji
    try:
        import morfeusz2
        print("âœ… Morfeusz2 juÅ¼ dostÄ™pny")
        return True
    except ImportError:
        pass

    # Metoda 2: pip install morfeusz2
    try:
        print("   ðŸ“¦ Metoda 1: pip install morfeusz2...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "morfeusz2"])
        import morfeusz2
        print("âœ… Morfeusz2 zainstalowany przez pip")
        return True
    except:
        print("   âŒ Metoda 1 nieudana")

    # Metoda 3: BezpoÅ›redni wheel
    wheel_urls = [
        "https://github.com/morfeusz-project/morfeusz/releases/download/2.1.9/morfeusz2-2.1.9-py3-none-any.whl",
        "https://github.com/morfeusz-project/morfeusz/releases/download/2.0.9/morfeusz2-2.0.9-py3-none-any.whl"
    ]

    for url in wheel_urls:
        try:
            print(f"   ðŸ“¦ Metoda 2: PrÃ³bujÄ™ {url}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", url])
            import morfeusz2
            print("âœ… Morfeusz2 zainstalowany z wheel")
            return True
        except Exception as e:
            print(f"   âŒ BÅ‚Ä…d: {str(e)[:100]}")
            continue

    # Metoda 4: Instalacja zaleÅ¼noÅ›ci systemowych
    try:
        print("   ðŸ“¦ Metoda 3: InstalujÄ™ zaleÅ¼noÅ›ci systemowe...")
        subprocess.check_call(["apt-get", "update", "-qq"])
        subprocess.check_call(["apt-get", "install", "-y", "-qq", "python3-dev", "build-essential"])
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "morfeusz2"])
        import morfeusz2
        print("âœ… Morfeusz2 zainstalowany z zaleÅ¼noÅ›ciami")
        return True
    except:
        print("   âŒ Metoda 3 nieudana")

    print("âš ï¸ Morfeusz2 niedostÄ™pny - system bÄ™dzie uÅ¼ywaÅ‚ fallback")
    return False

def install_spacy_polish():
    """Instalacja spaCy z polskim modelem."""
    print("ðŸ§  InstalujÄ™ spaCy...")

    try:
        import spacy
        # SprawdÅº czy model polski jest dostÄ™pny
        try:
            nlp = spacy.load("pl_core_news_sm")
            print("âœ… spaCy z modelem polskim juÅ¼ dostÄ™pny")
            return True
        except:
            pass
    except ImportError:
        # Zainstaluj spaCy
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "spacy"])
            import spacy
        except:
            print("âŒ Nie moÅ¼na zainstalowaÄ‡ spaCy")
            return False

    # Zainstaluj model polski
    models_to_try = ["pl_core_news_sm", "pl_core_news_md", "pl_core_news_lg"]

    for model in models_to_try:
        try:
            print(f"   ðŸ“š Pobieram model {model}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model, "--quiet"])

            # Test czy model dziaÅ‚a
            import spacy
            nlp = spacy.load(model)
            print(f"âœ… spaCy model {model} zainstalowany")
            return True
        except Exception as e:
            print(f"   âŒ Model {model} nieudany: {str(e)[:50]}")
            continue

    print("âš ï¸ spaCy zainstalowany ale bez polskiego modelu")
    return False

def install_stanza_polish():
    """Instalacja Stanza (opcjonalna)."""
    print("ðŸ“š InstalujÄ™ Stanza...")

    try:
        # Zainstaluj Stanza
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "stanza"])

        # SprawdÅº czy dziaÅ‚a
        import stanza
        print("âœ… Stanza zainstalowana (model zostanie pobrany przy pierwszym uÅ¼yciu)")
        return True
    except Exception as e:
        print(f"âŒ Stanza installation failed: {str(e)[:50]}")
        return False

# Uruchom instalacjÄ™ na poczÄ…tku komÃ³rki
print("=" * 60)
print("GTMÃ˜ POLISH NLP - NAPRAWIONA INSTALACJA")
print("=" * 60)

# Store installation status globally for access in other parts of the script
global installation_status
installation_status = install_dependencies()


# ============================================================================
# IMPORTY I KONFIGURACJA
# ============================================================================

import numpy as np
import pandas as pd

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("âš ï¸ Plotly niedostÄ™pny - wizualizacje wyÅ‚Ä…czone")

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# PODSTAWOWE KLASY GTMÃ˜
# ============================================================================

PHI = (1 + np.sqrt(5)) / 2  # Golden ratio
SQRT_2_INV = 1 / np.sqrt(2)  # Quantum amplitude
COGNITIVE_CENTER = np.array([0.5, 0.5, 0.5])  # Neutral knowledge state

@dataclass
class GTMOCoordinates:
    """WspÃ³Å‚rzÄ™dne GTMÃ˜ w przestrzeni fazowej 3D."""
    determination: float = 0.5
    stability: float = 0.5
    entropy: float = 0.5

    def to_array(self) -> np.ndarray:
        return np.array([self.determination, self.stability, self.entropy])

    def distance_to(self, other: 'GTMOCoordinates') -> float:
        return np.linalg.norm(self.to_array() - other.to_array())

@dataclass
class Configuration:
    """Kompletna konfiguracja w Przestrzeni Konfiguracyjnej GTMÃ˜."""
    position: np.ndarray = field(default_factory=lambda: np.zeros(3))
    time: float = 0.0
    orientation: np.ndarray = field(default_factory=lambda: np.array([0., 0., 0.]))
    scale: float = 1.0

    def to_dict(self) -> Dict:
        return {
            'position': self.position.tolist(),
            'time': self.time,
            'orientation': self.orientation.tolist(),
            'scale': self.scale
        }

class PolishCase(Enum):
    """Polskie przypadki gramatyczne z wspÃ³Å‚rzÄ™dnymi GTMÃ˜."""
    NOMINATIVE = ("nom", "mianownik", 0.901, 0.799, 0.151)
    GENITIVE = ("gen", "dopeÅ‚niacz", 0.701, 0.751, 0.301)
    DATIVE = ("dat", "celownik", 0.651, 0.549, 0.451)
    ACCUSATIVE = ("acc", "biernik", 0.851, 0.801, 0.201)
    INSTRUMENTAL = ("inst", "narzÄ™dnik", 0.499, 0.301, 0.899)
    LOCATIVE = ("loc", "miejscownik", 0.501, 0.499, 0.801)
    VOCATIVE = ("voc", "woÅ‚acz", 0.149, 0.151, 0.851)

    def __init__(self, tag: str, polish_name: str,
                 determination: float, stability: float, entropy: float):
        self.tag = tag
        self.polish_name = polish_name
        self.coords = GTMOCoordinates(determination, stability, entropy)

class PolishPOS(Enum):
    """Polish parts of speech with GTMÃ˜ coordinates."""
    SUBST = ("subst", "rzeczownik", 0.8, 0.9, 0.2)
    ADJ = ("adj", "przymiotnik", 0.6, 0.5, 0.4)
    ADV = ("adv", "przysÅ‚Ã³wek", 0.5, 0.6, 0.4)
    VERB = ("verb", "czasownik", 0.7, 0.4, 0.5)
    NUM = ("num", "liczebnik", 0.9, 0.8, 0.1)
    PRON = ("pron", "zaimek", 0.8, 0.6, 0.3)
    PREP = ("prep", "przyimek", 0.6, 0.8, 0.3)
    CONJ = ("conj", "spÃ³jnik", 0.5, 0.7, 0.4)
    PART = ("part", "partykuÅ‚a", 0.3, 0.2, 0.8)
    INTERP = ("interp", "interpunkcja", 0.9, 0.9, 0.1)

    def __init__(self, tag: str, polish_name: str,
                 determination: float, stability: float, entropy: float):
        self.tag = tag
        self.polish_name = polish_name
        self.coords = GTMOCoordinates(determination, stability, entropy)

class KnowledgeAttractor(Enum):
    """Atraktory wiedzy GTMÃ˜ do klasyfikacji."""
    PARTICLE = ("Î¨á´·", "Knowledge Particle", 0.85, 0.85, 0.15)
    SHADOW = ("Î¨Ê°", "Knowledge Shadow", 0.15, 0.15, 0.85)
    EMERGENT = ("Î¨á´º", "Emergent", 0.5, 0.3, 0.9)
    SINGULARITY = ("Ã˜", "Singularity", 1.0, 1.0, 0.0)
    FLUX = ("Î¨~", "Flux", 0.5, 0.5, 0.8)
    VOID = ("Î¨â—Š", "Void", 0.0, 0.0, 0.5)

    def __init__(self, symbol: str, display_name: str,
                 determination: float, stability: float, entropy: float):
        self.symbol = symbol
        self.display_name = display_name
        self.coords = GTMOCoordinates(determination, stability, entropy)

# ============================================================================
# ANALIZATOR MORFOLOGICZNY Z FALLBACK
# ============================================================================

class MorfeuszAnalyzer:
    """Analizator morfologiczny Morfeusz2 z fallback - TYLKO MORFOLOGIA."""

    def __init__(self):
        # Check global installation status
        if installation_status.get('morfeusz'):
            try:
                import morfeusz2
                self.morfeusz = morfeusz2.Morfeusz()
                self.available = True
                logger.info("âœ… Morfeusz2 zainicjalizowany")
            except Exception as e:
                self.morfeusz = None
                self.available = False
                logger.warning(f"âŒ Morfeusz2 bÅ‚Ä…d: {e}")
        else:
            self.morfeusz = None
            self.available = False
            logger.info("â„¹ï¸ Morfeusz2 niedostÄ™pny - uÅ¼ywam fallback")

    def analyze(self, text: str) -> List[Dict[str, Any]]:
        """Analiza morfologiczna z fallback."""
        if not self.available:
            return self._fallback_analysis(text)

        try:
            analysis = self.morfeusz.analyse(text)
            results = []

            for segment in analysis:
                start_idx, end_idx, interpretation = segment
                form, lemma, tag, labels, qualifiers = interpretation

                results.append({
                    'form': form,
                    'lemma': lemma,
                    'tag': tag,
                    'labels': labels if labels else [],
                    'qualifiers': qualifiers if qualifiers else [],
                    'start': start_idx,
                    'end': end_idx,
                    'source': 'morfeusz2'
                })

            return results
        except Exception as e:
            logger.warning(f"Morfeusz bÅ‚Ä…d: {e}, uÅ¼ywam fallback")
            return self._fallback_analysis(text)

    def _fallback_analysis(self, text: str) -> List[Dict[str, Any]]:
        """Prosta analiza morfologiczna fallback dla polskiego."""
        words = text.split()
        results = []

        for i, word in enumerate(words):
            # Prosta heurystyka morfologiczna dla polskiego
            lemma = self._guess_lemma(word)
            tag = self._guess_morphological_tag(word)

            results.append({
                'form': word,
                'lemma': lemma,
                'tag': tag,
                'labels': [],
                'qualifiers': [],
                'start': i,
                'end': i + 1,
                'source': 'fallback'
            })

        return results

    def _guess_lemma(self, word: str) -> str:
        """Prosta heurystyka lematyzacji."""
        word_lower = word.lower()

        # Usuwanie prostych koÅ„cÃ³wek fleksyjnych
        if word_lower.endswith(('em', 'ie', 'Ä…', 'y', 'e')):
            return word_lower[:-1]
        elif word_lower.endswith(('ami', 'ach', 'Ã³w')):
            return word_lower[:-2]
        else:
            return word_lower

    def _guess_morphological_tag(self, word: str) -> str:
        """Rozszerzona heurystyka tagÃ³w morfologicznych."""
        word_lower = word.lower()

        # Czasowniki
        if word_lower.endswith(('aÄ‡', 'eÄ‡', 'iÄ‡', 'owaÄ‡', 'nÄ…Ä‡')):
            return 'verb:inf'
        elif word_lower.endswith(('Ä™', 'esz', 'e', 'emy', 'ecie', 'Ä…')):
            return 'verb:fin:sg:3:pres'

        # Rzeczowniki
        elif word_lower.endswith(('oÅ›Ä‡', 'enie', 'anie', 'cie')):
            return 'subst:sg:nom:f'
        elif word_lower.endswith(('ek', 'ik', 'ar', 'arz')):
            return 'subst:sg:nom:m1'
        elif word_lower.endswith(('a', 'ka', 'ia')):
            return 'subst:sg:nom:f'
        elif word_lower.endswith(('o', 'um', 'e')):
            return 'subst:sg:nom:n'

        # Przymiotniki
        elif word_lower.endswith(('ny', 'ty', 'Å‚y', 'wy', 'owy', 'owy')):
            return 'adj:sg:nom:m1:pos'
        elif word_lower.endswith(('na', 'ta', 'Å‚a', 'wa', 'owa')):
            return 'adj:sg:nom:f:pos'
        elif word_lower.endswith(('ne', 'te', 'Å‚e', 'we', 'owe')):
            return 'adj:sg:nom:n:pos'

        # PrzysÅ‚Ã³wki
        elif word_lower.endswith(('sko', 'Å›nie', 'Ä…co', 'nie')):
            return 'adv:pos'

        # SpÃ³jniki i partykuÅ‚y
        elif word_lower in ['i', 'a', 'ale', 'Å¼e', 'bo', 'oraz', 'lub']:
            return 'conj'
        elif word_lower in ['nie', 'tak', 'juÅ¼', 'jeszcze', 'bardzo']:
            return 'part'

        # Przyimki
        elif word_lower in ['w', 'na', 'do', 'z', 'o', 'od', 'po', 'za', 'przed']:
            return 'prep:inst'

        # DomyÅ›lnie rzeczownik
        else:
            return 'subst:sg:nom:m3'

class SpacyAnalyzer:
    """spaCy analyzer for Polish syntactic analysis with fallback."""

    def __init__(self):
         # Check global installation status
        if installation_status.get('spacy'):
            try:
                import spacy
                self.nlp = spacy.load("pl_core_news_lg") # Try lg first
                self.available = True
                logger.info("âœ… spaCy Polish model loaded")
            except:
                 try:
                     self.nlp = spacy.load("pl_core_news_md") # Try md
                     self.available = True
                     logger.info("âœ… spaCy Polish model loaded (md)")
                 except:
                     try:
                         self.nlp = spacy.load("pl_core_news_sm") # Try sm
                         self.available = True
                         logger.info("âœ… spaCy Polish model loaded (sm)")
                     except Exception as e:
                         self.nlp = None
                         self.available = False
                         logger.warning(f"âŒ spaCy not available: {e}")
        else:
            self.nlp = None
            self.available = False
            logger.info("â„¹ï¸ spaCy niedostÄ™pny - uÅ¼ywam fallback")


    def analyze(self, text: str) -> Optional[Any]:
        """Analyze text with spaCy or fallback."""
        if not self.available:
            return self._fallback_analysis(text)

        try:
            return self.nlp(text)
        except Exception as e:
            logger.warning(f"spaCy bÅ‚Ä…d: {e}, uÅ¼ywam fallback")
            return self._fallback_analysis(text)

    def _fallback_analysis(self, text: str) -> Optional[Any]:
        """Prosta analiza fallback spaCy (syntaktyka)."""
        # Create a dummy Doc-like object for fallback
        class DummyMorph:
            """Dummy class to mimic spaCy Morph object with .get method returning a list."""
            def __init__(self, data: Dict = None):
                self._data = data if data is not None else {}

            def get(self, key, default=None):
                # Return a list or default, mimicking spaCy's behavior
                return [self._data[key]] if key in self._data else default


        class DummyToken:
            def __init__(self, text, pos, dep, lemma, morph_data: Dict = None, is_stop=False, head=None, children=None):
                self.text = text
                self.pos_ = pos
                self.dep_ = dep
                self.lemma_ = lemma
                # Initialize morph as a DummyMorph object
                self.morph = DummyMorph(morph_data)
                self.is_stop = is_stop
                self.head = head if head is not None else self
                self.children = children if children is not None else []
                self.has_vector = False
                self.vector = None

            def __str__(self):
                return self.text

            @property
            def ancestors(self):
                # Simplified: assume max depth 2 for fallback
                return [self.head] if self.head != self else []


        class DummyDoc:
            def __init__(self, text):
                self.text = text
                words = text.split()
                self.tokens = []
                root_found = False
                for i, word in enumerate(words):
                    # Simple heuristic for POS and dependency
                    pos = 'X'
                    dep = 'dep'
                    lemma = word.lower()
                    morph_data = {} # Default empty morph data

                    if word.lower() in ['jest', 'sÄ…', 'byÄ‡']:
                        pos = 'VERB'
                        dep = 'ROOT'
                        root_found = True
                        # Add dummy morph data for verb
                        morph_data = {"Aspect": ["Imp"], "Mood": ["Ind"]}
                    elif word.lower() in ['i', 'a', 'ale']:
                         pos = 'CCONJ'
                    elif word.lower() in ['.', ',', '!', '?']:
                         pos = 'PUNCT'

                    # Pass dummy morph data
                    self.tokens.append(DummyToken(word, pos, dep, lemma, morph_data=morph_data))

                # Basic dependency linkage for fallback
                if root_found:
                    root_token = next((t for t in self.tokens if t.dep_ == 'ROOT'), None)
                    if root_token:
                        for token in self.tokens:
                            if token != root_token:
                                token.head = root_token # Link all others to root


            def __iter__(self):
                return iter(self.tokens)

            def __len__(self):
                return len(self.tokens)

        logger.info("Using spaCy fallback analysis.")
        return DummyDoc(text)

class StanzaAnalyzer:
    """Stanza analyzer for Polish with fallback."""

    def __init__(self):
         # Check global installation status
        if installation_status.get('stanza'):
            try:
                import stanza
                # Download model if needed (handled in install_dependencies)
                self.nlp = stanza.Pipeline('pl',
                                        processors='tokenize,pos,lemma,depparse',
                                        verbose=False,
                                        use_gpu=False)
                self.available = True
                logger.info("âœ… Stanza Polish pipeline loaded")
            except Exception as e:
                self.nlp = None
                self.available = False
                logger.warning(f"âŒ Stanza not available: {e}")
        else:
            self.nlp = None
            self.available = False
            logger.info("â„¹ï¸ Stanza niedostÄ™pny - uÅ¼ywam fallback")


    def analyze(self, text: str) -> Optional[Any]:
        """Analyze text with Stanza or fallback."""
        if not self.available:
            return self._fallback_analysis(text)

        try:
            doc = self.nlp(text)
            return doc
        except Exception as e:
            logger.warning(f"Stanza bÅ‚Ä…d: {e}, uÅ¼ywam fallback")
            return self._fallback_analysis(text)

    def _fallback_analysis(self, text: str) -> Optional[Any]:
        """Prosta analiza fallback Stanza."""
        logger.info("Using Stanza fallback analysis.")
        # Return a simple structure, possibly mirroring the Stanza Doc structure partially
        class DummyStanzaWord:
            def __init__(self, text, lemma, upos, xpos, feats, head, deprel):
                self.text = text
                self.lemma = lemma
                self.upos = upos
                self.xpos = xpos
                self.feats = feats
                self.head = head
                self.deprel = deprel

        class DummyStanzaSentence:
            def __init__(self, text):
                self.text = text
                words = text.split()
                self.words = []
                for word in words:
                    # Simple heuristic
                    lemma = word.lower()
                    upos = 'X'
                    xpos = '_'
                    feats = '_'
                    head = 0 # Simplified head (0 for root)
                    deprel = 'root' if head == 0 else 'dep'
                    self.words.append(DummyStanzaWord(word, lemma, upos, xpos, feats, head, deprel))

        class DummyStanzaDoc:
            def __init__(self, text):
                self.text = text
                self.sentences = [DummyStanzaSentence(text)] # One sentence for simplicity

        return DummyStanzaDoc(text)


# ============================================================================
# GÅÃ“WNY PROCESOR GTMÃ˜
# ============================================================================

class GTMOProcessor:
    """GÅ‚Ã³wny procesor analizy GTMÃ˜."""

    def __init__(self):
        print("ðŸ”§ InicjalizujÄ™ procesor GTMÃ˜...")
        self.morfeusz = MorfeuszAnalyzer()
        self.spacy = SpacyAnalyzer()
        self.stanza = StanzaAnalyzer() # Added Stanza initialization
        self._init_mappings()
        print("âœ… Procesor GTMÃ˜ gotowy")

    def _init_mappings(self):
        """Inicjalizacja mapowaÅ„ morfologicznych."""
        self.case_map = {case.tag: case for case in PolishCase}
        # Mapowanie POS tagÃ³w spaCy/Stanza na enum PolishPOS
        self.pos_map = {
            'NOUN': PolishPOS.SUBST,
            'ADJ': PolishPOS.ADJ,
            'VERB': PolishPOS.VERB,
            'ADV': PolishPOS.ADV,
            'NUM': PolishPOS.NUM,
            'PRON': PolishPOS.PRON,
            'ADP': PolishPOS.PREP,
            'CCONJ': PolishPOS.CONJ,
            'SCONJ': PolishPOS.CONJ,
            'PART': PolishPOS.PART,
            'PUNCT': PolishPOS.INTERP,
            'DET': PolishPOS.PRON, # Mapping DET to PRON as it's not in PolishPOS enum
            'X': PolishPOS.PART # Mapping X to PART as a general fallback
        }


    def calculate_coordinates(self, text: str) -> Tuple[GTMOCoordinates, Configuration, Dict]:
        """Oblicz wspÃ³Å‚rzÄ™dne GTMÃ˜ dla tekstu."""
        config = Configuration()
        metadata = {'text': text, 'analyses': [], 'morphological_features': []}

        # Ternary Stream 1: Morphological analysis (Morfeusz or fallback)
        morfeusz_analysis = self.morfeusz.analyze(text)
        morph_coords = self._process_morphology(morfeusz_analysis, metadata)

        # Ternary Stream 2: Syntactic analysis (spaCy or fallback)
        spacy_doc = self.spacy.analyze(text)
        syntax_coords, config = self._process_syntax(spacy_doc, metadata) # Pass spacy doc

        # Ternary Stream 3: Semantic computation (Simplified)
        semantic_coords = self._compute_semantics(text)

        # Combine three streams with weighted average
        weights = [0.4, 0.35, 0.25]  # Morphology, Syntax, Semantics
        combined = np.average(
            [morph_coords.to_array(), syntax_coords.to_array(), semantic_coords.to_array()],
            weights=weights,
            axis=0
        )

        coords = GTMOCoordinates(*combined)
        # config.position and config.scale set in _process_syntax
        config.time = datetime.now().timestamp()


        # Classify to nearest attractor
        metadata['attractor'] = self._find_nearest_attractor(coords)

        return coords, config, metadata

    def _process_morphology(self, morfeusz_analysis: List[Dict], metadata: Dict) -> GTMOCoordinates:
        """Przetwarzanie cech morfologicznych."""
        coords_list = []

        for analysis in morfeusz_analysis:
            tag_parts = analysis.get('tag', '').split(':')

            # WyciÄ…gnij przypadek
            for tag in tag_parts:
                if tag in self.case_map:
                    case = self.case_map[tag]
                    coords_list.append(case.coords.to_array())
                    metadata['analyses'].append({
                        'type': 'case',
                        'value': case.polish_name,
                        'source': analysis.get('source', 'morfeusz2' if self.morfeusz.available else 'fallback')
                    })
                    break

        if coords_list:
            return GTMOCoordinates(*np.mean(coords_list, axis=0))
        return GTMOCoordinates(0.5, 0.5, 0.5)

    def _process_syntax(self, spacy_doc: Optional[Any], metadata: Dict) -> Tuple[GTMOCoordinates, Configuration]:
        """Przetwarzanie cech syntaktycznych."""
        coords_list = []
        config = Configuration() # Initialize config here

        if spacy_doc:
            try:
                # Analyze dependency tree structure
                dependency_scores = {
                    'ROOT': (0.9, 0.8, 0.1),      # Root verb - high determination
                    'nsubj': (0.8, 0.7, 0.2),     # Nominal subject
                    'obj': (0.7, 0.6, 0.3),       # Direct object
                    'iobj': (0.6, 0.5, 0.4),      # Indirect object
                    'ccomp': (0.5, 0.4, 0.5),     # Clausal complement
                    'xcomp': (0.5, 0.4, 0.5),     # Open clausal complement
                    'advcl': (0.4, 0.3, 0.6),     # Adverbial clause
                    'advmod': (0.3, 0.4, 0.6),    # Adverbial modifier
                    'amod': (0.6, 0.5, 0.4),      # Adjectival modifier
                    'nmod': (0.5, 0.6, 0.4),      # Nominal modifier
                    'det': (0.8, 0.8, 0.2),       # Determiner
                    'case': (0.7, 0.7, 0.3),      # Case marking
                    'punct': (0.9, 0.9, 0.1),     # Punctuation
                    'conj': (0.5, 0.5, 0.5),      # Conjunction
                    'cc': (0.5, 0.5, 0.5),        # Coordinating conjunction
                    'aux': (0.6, 0.6, 0.4),       # Auxiliary
                    'cop': (0.7, 0.7, 0.3),       # Copula
                    'mark': (0.4, 0.4, 0.6),      # Marker
                    'appos': (0.5, 0.4, 0.5),     # Appositional modifier
                    'acl': (0.4, 0.3, 0.6),       # Adjectival clause
                    'dep': (0.3, 0.3, 0.7)        # Unspecified dependency
                }

                root_count = 0
                total_depth = 0
                max_depth = 0

                for token in spacy_doc:
                    # Map POS tags
                    if token.pos_ in self.pos_map:
                        pos_enum = self.pos_map[token.pos_]
                        coords_list.append(pos_enum.coords.to_array())
                        metadata['analyses'].append({
                            'type': 'pos',
                            'value': pos_enum.polish_name, # Correctly access polish_name from enum member
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })
                    elif token.pos_ != 'SPACE': # Ignore whitespace tokens
                         metadata['analyses'].append({
                            'type': 'pos',
                            'value': token.pos_, # Use raw POS if not in map
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })


                    # Analyze dependency relations
                    if token.dep_ in dependency_scores:
                        dep_coords = dependency_scores[token.dep_]
                        coords_list.append(np.array(dep_coords))
                        metadata['analyses'].append({
                            'type': 'dependency',
                            'value': token.dep_,
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })
                    elif token.dep_ != 'punct': # Ignore punctuation dependencies
                         metadata['analyses'].append({
                            'type': 'dependency',
                            'value': token.dep_,
                            'token': token.text,
                            'source': 'spacy' if self.spacy.available else 'fallback'
                        })


                    # Calculate syntactic tree depth
                    # Check if token has head before accessing ancestors
                    if token.head != token:
                        ancestors = list(token.ancestors)
                        depth = len(ancestors)
                        total_depth += depth
                        max_depth = max(max_depth, depth)

                    # Special handling for ROOT
                    if token.dep_ == "ROOT":
                        root_count += 1
                        config.position[0] = 0.8  # High semantic centrality

                        # Analyze verb properties for Polish
                        if token.pos_ == "VERB":
                            # Safely access morph attributes and handle lists
                            aspect_data = None
                            mood_data = None
                            if hasattr(token, 'morph') and token.morph is not None:
                                 # Use .get() to retrieve the list from the morph object
                                 if hasattr(token.morph, 'get'): # spaCy Morph or similar with .get
                                     aspect_data = token.morph.get("Aspect")
                                     mood_data = token.morph.get("Mood")
                                 elif isinstance(token.morph, dict): # Fallback case
                                     aspect_data = token.morph.get("Aspect")
                                     mood_data = token.morph.get("Mood")


                            if aspect_data and isinstance(aspect_data, list) and len(aspect_data) > 0: # Check if list and not empty
                                aspect = str(aspect_data[0]) # Access element by index
                                if aspect == "Perf":
                                    config.position[1] = 0.7  # Perfective - more determined
                                else:
                                    config.position[1] = 0.4  # Imperfective - less determined

                            if mood_data and isinstance(mood_data, list) and len(mood_data) > 0: # Check if list and not empty
                                mood = str(mood_data[0]) # Access element by index
                                mood_values = {
                                    "Ind": 0.8,  # Indicative - factual
                                    "Imp": 0.6,  # Imperative - directive
                                    "Cnd": 0.3   # Conditional - hypothetical
                                }
                                config.position[2] = mood_values.get(mood, 0.5)


                    # Analyze Polish-specific features (integrated into the main loop)
                    if token.pos_ == "ADP":  # Preposition
                        # Different prepositions require different cases
                        prep_case_patterns = {
                            'w': (0.6, 0.7, 0.3),    # w + locative
                            'na': (0.7, 0.6, 0.3),   # na + accusative/locative
                            'z': (0.5, 0.5, 0.5),    # z + genitive/instrumental
                            'do': (0.8, 0.7, 0.2),   # do + genitive
                            'od': (0.7, 0.6, 0.3),   # od + genitive
                            'przez': (0.6, 0.5, 0.4), # przez + accusative
                            'dla': (0.6, 0.6, 0.4),  # dla + genitive
                            'o': (0.5, 0.4, 0.5),    # o + locative/accusative
                            'po': (0.4, 0.3, 0.6),   # po + locative/accusative
                            'za': (0.5, 0.5, 0.5)    # za + instrumental/accusative
                        }
                        if token.text.lower() in prep_case_patterns:
                            coords_list.append(np.array(prep_case_patterns[token.text.lower()]))


                # Calculate average tree depth for abstraction scale
                avg_depth = total_depth / len(spacy_doc) if len(spacy_doc) > 0 else 1
                config.scale = min(1.0, avg_depth / 5.0)

                # Analyze sentence complexity
                if len(spacy_doc) > 0:
                    # Simple sentence vs complex sentence
                    if root_count == 1 and max_depth <= 2:
                        # Simple sentence - higher stability
                        coords_list.append(np.array([0.7, 0.8, 0.2]))
                    elif root_count > 1 or max_depth > 4:
                        # Complex sentence - higher entropy
                        coords_list.append(np.array([0.4, 0.3, 0.7]))
                    else:
                        # Medium complexity
                        coords_list.append(np.array([0.5, 0.5, 0.5]))


            except Exception as e:
                logger.warning(f"BÅ‚Ä…d przetwarzania spaCy: {e}")

        if coords_list:
            coords = GTMOCoordinates(*np.mean(coords_list, axis=0))
        else:
            coords = GTMOCoordinates(0.5, 0.5, 0.5)

        return coords, config

    def _compute_semantics(self, text: str) -> GTMOCoordinates:
        """Oblicz cechy semantyczne."""
        # Simplified semantic analysis
        text_lower = text.lower()

        determination = 0.5
        stability = 0.5
        entropy = 0.5

        # Count sentence type indicators
        if '?' in text:
            entropy += 0.3
            stability -= 0.2
            determination -= 0.1
        elif '!' in text:
            determination += 0.1
            entropy += 0.1
        elif '.' in text:
            stability += 0.1
            determination += 0.05

        # Simple sentiment check (very basic)
        positive_words = ['dobry', 'miÅ‚y', 'szczÄ™Å›cie', 'radoÅ›Ä‡', 'piÄ™kny']
        negative_words = ['zÅ‚y', 'smutny', 'bÃ³l', 'strach', 'Å›mierÄ‡']

        sentiment_score = 0
        for word in text_lower.split():
            if word in positive_words:
                sentiment_score += 1
            elif word in negative_words:
                sentiment_score -= 1

        if sentiment_score > 0:
            determination += 0.1 # Positive sentiment adds some clarity
        elif sentiment_score < 0:
            entropy += 0.1 # Negative sentiment might add complexity

        # Clip to valid range
        coords = GTMOCoordinates(
            np.clip(determination, 0, 1),
            np.clip(stability, 0, 1),
            np.clip(entropy, 0, 1)
        )

        return coords


    def _find_nearest_attractor(self, coords: GTMOCoordinates) -> Dict:
        """ZnajdÅº najbliÅ¼szy atraktor wiedzy."""
        min_distance = float('inf')
        nearest = None

        for attractor in KnowledgeAttractor:
            distance = coords.distance_to(attractor.coords)
            if distance < min_distance:
                min_distance = distance
                nearest = attractor

        return {
            'symbol': nearest.symbol,
            'name': nearest.display_name,
            'distance': float(min_distance),
            'coordinates': {
                'determination': nearest.coords.determination,
                'stability': nearest.coords.stability,
                'entropy': nearest.coords.entropy
            }
        }

# ============================================================================
# WIZUALIZATOR Z FALLBACK
# ============================================================================

class GTMOVisualizer:
    """Silnik wizualizacji wynikÃ³w analizy GTMÃ˜."""

    def __init__(self):
        self.available = True # Assume Plotly is available after installation attempt
        try:
            import plotly.graph_objects as go
            import plotly.express as px
            from plotly.subplots import make_subplots
        except ImportError:
            self.available = False
            logger.warning("Plotly niedostÄ™pny - wizualizacje wyÅ‚Ä…czone")


        self.attractor_colors = {
            'Î¨á´·': '#FF6B6B',  # Red
            'Î¨Ê°': '#4ECDC4',  # Cyan
            'Î¨á´º': '#45B7D1',  # Blue
            'Ã˜': '#96CEB4',   # Green
            'Î¨~': '#FECA57',  # Yellow
            'Î¨â—Š': '#DDA0DD'   # Plum
        }

    def create_configuration_space_plot(self, results: List[Dict]):
        """UtwÃ³rz interaktywnÄ… wizualizacjÄ™ 3D Przestrzeni Konfiguracyjnej."""
        if not self.available:
            print("âŒ Plotly niedostÄ™pny - nie moÅ¼na utworzyÄ‡ wizualizacji")
            return None

        try:
            fig = make_subplots(
                rows=2, cols=2,
                specs=[
                    [{'type': 'scatter3d', 'rowspan': 2}, {'type': 'scatter'}],
                    [None, {'type': 'scatter'}]
                ],
                subplot_titles=(
                    'PrzestrzeÅ„ Konfiguracyjna (3D)',
                    'Determinacja vs StabilnoÅ›Ä‡',
                    'RozkÅ‚ad Entropii'
                )
            )

            # WyciÄ…gnij dane
            sentences = [r['text'] for r in results]
            coords = [r['coordinates'] for r in results]
            attractors = [r['metadata']['attractor'] for r in results]

            # Wykres 3D scatter
            x = [c['determination'] for c in coords]
            y = [c['stability'] for c in coords]
            z = [c['entropy'] for c in coords]
            colors = [self.attractor_colors.get(a['symbol'], '#808080') for a in attractors]

            # Dodaj punkty zdaÅ„
            fig.add_trace(
                go.Scatter3d(
                    x=x, y=y, z=z,
                    mode='markers+text',
                    marker=dict(
                        size=10,
                        color=colors,
                        opacity=0.8,
                        line=dict(width=1, color='white')
                    ),
                    text=[f"{i+1}" for i in range(len(sentences))],
                    textposition='top center',
                    hovertemplate='<b>Zdanie %{text}</b><br>' +
                                  'Determinacja: %{x:.3f}<br>' +
                                  'StabilnoÅ›Ä‡: %{y:.3f}<br>' +
                                  'Entropia: %{z:.3f}<br>' +
                                  '<extra></extra>',
                    name='Zdania'
                ),
                row=1, col=1
            )

            # Dodaj atraktory
            for attractor in KnowledgeAttractor:
                fig.add_trace(
                    go.Scatter3d(
                        x=[attractor.coords.determination],
                        y=[attractor.coords.stability],
                        z=[attractor.coords.entropy],
                        mode='markers+text',
                        marker=dict(
                            size=20,
                            color=self.attractor_colors.get(attractor.symbol, '#808080'),
                            symbol='diamond',
                            opacity=1.0,
                            line=dict(width=2, color='black')
                        ),
                        text=[attractor.symbol],
                        textposition='top center',
                        name=attractor.display_name,
                        hovertemplate=f'<b>{attractor.display_name}</b><br>' +
                                      'D: %{x:.3f}<br>S: %{y:.3f}<br>E: %{z:.3f}<br>' +
                                      '<extra></extra>'
                    ),
                    row=1, col=1
                )

            # Projekcja 2D: Determinacja vs StabilnoÅ›Ä‡
            fig.add_trace(
                go.Scatter(
                    x=x, y=y,
                    mode='markers',
                    marker=dict(
                        size=8,
                        color=z,
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title='Entropia', x=1.15)
                    ),
                    text=[f"S{i+1}" for i in range(len(sentences))],
                    hovertemplate='<b>%{text}</b><br>' +
                                  'D: %{x:.3f}<br>S: %{y:.3f}<br>' +
                                  '<extra></extra>',
                    showlegend=False
                ),
                row=1, col=2
            )

            # RozkÅ‚ad entropii
            fig.add_trace(
                go.Histogram(
                    x=z,
                    nbinsx=20,
                    marker_color='#45B7D1',
                    opacity=0.7,
                    showlegend=False
                ),
                row=2, col=2
            )

            # Actual subplot axis titles based on make_subplots structure
            fig.update_layout(
                title={
                    'text': 'Analiza Przestrzeni Konfiguracyjnej GTMÃ˜',
                    'font': {'size': 20, 'color': '#2C3E50'}
                },
                showlegend=True,
                height=800,
                scene=dict(  # This is for the 3D plot (row 1, col 1)
                    xaxis_title='Determinacja â†’',
                    yaxis_title='StabilnoÅ›Ä‡ â†’',
                    zaxis_title='Entropia â†’',
                    camera=dict(
                        eye=dict(x=1.5, y=1.5, z=1.5)
                    )
                ),
                xaxis2_title='Determinacja', # Correct axis reference for row 1, col 2
                yaxis2_title='StabilnoÅ›Ä‡',   # Correct axis reference for row 1, col 2
                xaxis3=dict(title='Entropia'), # Correct way to set title for axis 3
                yaxis3=dict(title='Liczba')   # Correct way to set title for axis 3
            )

            return fig

        except Exception as e:
            logger.error(f"BÅ‚Ä…d tworzenia wizualizacji: {e}")
            return None

    def show_text_summary(self, results: List[Dict]):
        """PokaÅ¼ podsumowanie tekstowe gdy wizualizacje niedostÄ™pne."""
        print("\n" + "="*60)
        print("PODSUMOWANIE ANALIZY GTMÃ˜")
        print("="*60)

        for i, result in enumerate(results, 1):
            coords = result['coordinates']
            attractor = result['metadata']['attractor']

            print(f"\n{i}. '{result['text']}'")
            print(f"   ðŸ“Š WspÃ³Å‚rzÄ™dne: [{coords['determination']:.3f}, {coords['stability']:.3f}, {coords['entropy']:.3f}]")
            print(f"   ðŸŽ¯ Atraktor: {attractor['symbol']} ({attractor['name']})")
            print(f"   ðŸ“ Dystans: {attractor['distance']:.3f}")

        # Statystyki ogÃ³lne
        if results:
            avg_det = np.mean([r['coordinates']['determination'] for r in results])
            avg_stab = np.mean([r['coordinates']['stability'] for r in results])
            avg_ent = np.mean([r['coordinates']['entropy'] for r in results])

            print(f"\nðŸ“ˆ STATYSTYKI ÅšREDNIE:")
            print(f"   Determinacja: {avg_det:.3f}")
            print(f"   StabilnoÅ›Ä‡: {avg_stab:.3f}")
            print(f"   Entropia: {avg_ent:.3f}")
        else:
            print("\nðŸ“ˆ Brak wynikÃ³w do podsumowania.")

# ============================================================================
# GÅÃ“WNA KLASA ANALIZATORA
# ============================================================================

class GTMOAnalyzer:
    """GÅ‚Ã³wny analizator orkiestrujÄ…cy kompletny pipeline analizy GTMÃ˜."""
    def __init__(self):
        global installation_status

        logger.info("InicjalizujÄ™ Analizator GTMÃ˜...")

        # Check if installation status is available and indicates success
        # UÅ¼ywamy `globals()` do bezpiecznego sprawdzenia istnienia zmiennej
        if 'installation_status' not in globals() or not installation_status.get('morfeusz') or not installation_status.get('spacy') or not installation_status.get('stanza'):
            logger.warning("âš ï¸  ZaleÅ¼noÅ›ci nie zostaÅ‚y pomyÅ›lnie zainstalowane. Analiza moÅ¼e uÅ¼ywaÄ‡ fallbackÃ³w.")

            # PrÃ³ba ponownej instalacji
            installation_status = install_dependencies()

        self.processor = GTMOProcessor()
        self.visualizer = GTMOVisualizer()
        self.test_sentences = self._get_test_sentences()


    def _get_test_sentences(self) -> List[str]:
        """Pobierz 15 rÃ³Å¼norodnych polskich zdaÅ„ testowych."""
        return [
            # Fakty i pewnoÅ›ci
            "Warszawa jest stolicÄ… Polski.",
            "Dwa plus dwa rÃ³wna siÄ™ cztery.",

            # Pytania
            "Czy sprawiedliwoÅ›Ä‡ zawsze zwyciÄ™Å¼a?",
            "Kiedy nadejdzie prawdziwa wolnoÅ›Ä‡?",

            # NiepewnoÅ›Ä‡
            "MoÅ¼e jutro bÄ™dzie padaÄ‡ deszcz.",
            "Chyba nie zdÄ…Å¼ymy na pociÄ…g.",

            # PojÄ™cia abstrakcyjne
            "MiÅ‚oÅ›Ä‡ jest silniejsza niÅ¼ Å›mierÄ‡.",
            "SprawiedliwoÅ›Ä‡ powinna byÄ‡ Å›lepa.",

            # Paradoksy
            "To zdanie jest faÅ‚szywe.",
            "Wiem, Å¼e nic nie wiem.",

            # Polecenia
            "PrzyjdÅº jutro o Ã³smej rano!",
            "Nie zapomnij o spotkaniu.",

            # Emocjonalne
            "Och, jak piÄ™knie pachnÄ… te rÃ³Å¼e!",

            # ZÅ‚oÅ¼one
            "ChoÄ‡ burza szaleje, statek pÅ‚ynie dalej.",
            "Gdyby nie deszcz, poszlibyÅ›my na spacer."
        ]

    def analyze_all(self) -> List[Dict]:
        """Analizuj wszystkie zdania testowe."""
        results = []

        for i, sentence in enumerate(self.test_sentences, 1):
            logger.info(f"AnalizujÄ™ zdanie {i}/{len(self.test_sentences)}: {sentence[:50]}...")

            try:
                coords, config, metadata = self.processor.calculate_coordinates(sentence)

                result = {
                    'id': i,
                    'text': sentence,
                    'coordinates': {
                        'determination': float(coords.determination),
                        'stability': float(coords.stability),
                        'entropy': float(coords.entropy)
                    },
                    'configuration': config.to_dict(),
                    'metadata': metadata,
                    'timestamp': datetime.now().isoformat()
                }

                results.append(result)

            except Exception as e:
                logger.error(f"âŒ BÅ‚Ä…d podczas analizy zdania '{sentence[:50]}...': {e}")
                # Optionally append a failed analysis marker
                results.append({
                    'id': i,
                    'text': sentence,
                    'status': 'failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                continue


        return results

    def save_results(self, results: List[Dict]) -> str:
        """Zapisz wyniki do pliku JSON z timestamp."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'gtmo_analysis_{timestamp}.json'

        output = {
            'version': '2.1',
            'timestamp': datetime.now().isoformat(),
            'total_sentences': len([r for r in results if r.get('status') != 'failed']),
            'failed_sentences': len([r for r in results if r.get('status') == 'failed']),
            'installation_status': installation_status,
            'results': results,
            'statistics': self._calculate_statistics(results)
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"Wyniki zapisane do {filename}")
        return filename

    def _calculate_statistics(self, results: List[Dict]) -> Dict:
        """Oblicz statystyki podsumowujÄ…ce (tylko dla udanych analiz)."""
        successful_results = [r for r in results if r.get('status') != 'failed']

        if not successful_results:
             return {}

        coords = [r['coordinates'] for r in successful_results]

        return {
            'mean_determination': float(np.mean([c['determination'] for c in coords])),
            'mean_stability': float(np.mean([c['stability'] for c in coords])),
            'mean_entropy': float(np.mean([c['entropy'] for c in coords])),
            'std_determination': float(np.std([c['determination'] for c in coords])),
            'std_stability': float(np.std([c['stability'] for c in coords])),
            'std_entropy': float(np.std([c['entropy'] for c in coords])),
            'attractor_distribution': self._count_attractors(successful_results)
        }

    def _count_attractors(self, results: List[Dict]) -> Dict:
        """Policz rozkÅ‚ad najbliÅ¼szych atraktorÃ³w (tylko dla udanych analiz)."""
        attractors = {}
        for result in results:
            if 'metadata' in result and 'attractor' in result['metadata']:
                 symbol = result['metadata']['attractor']['symbol']
                 attractors[symbol] = attractors.get(symbol, 0) + 1
        return attractors


    def run_complete_analysis(self):
        """Uruchom kompletny pipeline analizy."""
        print("\n" + "="*60)
        print("ANALIZATOR GTMÃ˜ POLISH NLP")
        print("="*60 + "\n")

        # Analizuj zdania
        print("ðŸ” AnalizujÄ™ zdania...")
        results = self.analyze_all()

        # Zapisz wyniki
        if results:
            print("\nðŸ’¾ ZapisujÄ™ wyniki...")
            filename = self.save_results(results)
            print(f"âœ… Wyniki zapisane do: {filename}")

            # UtwÃ³rz wizualizacje (tylko dla udanych analiz)
            successful_results = [r for r in results if r.get('status') != 'failed']
            if successful_results:
                print("\nðŸ“Š GenerujÄ™ wizualizacje...")

                if self.visualizer.available:
                    fig1 = self.visualizer.create_configuration_space_plot(successful_results)

                    if fig1:
                        html_filename = 'gtmo_configuration_space.html'
                        fig1.write_html(html_filename)
                        print(f"âœ… Wizualizacja zapisana do {html_filename}")
                        fig1.show()
                    else:
                        print("âŒ BÅ‚Ä…d generowania wizualizacji")
                        self.visualizer.show_text_summary(successful_results)
                else:
                    print("â„¹ï¸ Plotly niedostÄ™pny - pokazujÄ™ podsumowanie tekstowe")
                    self.visualizer.show_text_summary(successful_results)
            else:
                 print("\nâ„¹ï¸ Brak udanych analiz do wizualizacji.")

        else:
             print("\nâŒ Brak wynikÃ³w analizy do zapisania lub wizualizacji.")


        print("\n" + "="*60)
        print("ANALIZA ZAKOÅƒCZONA")
        print("="*60)

        return results

# ============================================================================
# WYKONANIE
# ============================================================================

if __name__ == "__main__":
    # Inicjalizuj i uruchom analizator
    try:
        analyzer = GTMOAnalyzer()
        results = analyzer.run_complete_analysis()

        print(f"\nâœ… GTMÃ˜ Analiza zakoÅ„czona!")
        print(f"Przetworzono {len(results)} zdaÅ„ ({len([r for r in results if r.get('status') != 'failed'])} udanych)")
        print("SprawdÅº wygenerowane pliki dla szczegÃ³Å‚owych wynikÃ³w.")

    except Exception as e:
        print(f"\nâŒ Krytyczny bÅ‚Ä…d podczas inicjalizacji lub uruchamiania: {e}")
        import traceback
        traceback.print_exc()
